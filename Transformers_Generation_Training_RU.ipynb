{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers Generation Training RU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7DrdgrQB8ky",
        "colab_type": "code",
        "outputId": "8dbae1ef-c6d9-44ba-c783-61fefb5dda06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=48040ec63a9c62ec2c31101476eb5f45d9e8d595b9dbad788f9cc90328634e5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  | Proc size: 159.9 MB\n",
            "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhsqSD6GRE2R",
        "colab_type": "code",
        "outputId": "eda297bd-9731-4d0f-d9d6-4ee1b7097bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        }
      },
      "source": [
        "! pip install awscli\n",
        "! aws s3 sync --no-sign-request s3://models.dobro.ai/gpt2/ru/unfreeze_all gpt2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting awscli\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/55/7523657affeca13e76d99b8e859f9b92702be91a05f81839a9abd78062aa/awscli-1.18.53.tar.gz (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 2.8MB/s \n",
            "\u001b[?25hCollecting botocore==1.16.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/32/513589cba34b5413965f7486d2f72a4649df1430e73f02b4067011ddedc7/botocore-1.16.3-py2.py3-none-any.whl (6.2MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2MB 18.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli) (0.15.2)\n",
            "Collecting rsa<=3.5.0,>=3.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from awscli) (0.3.3)\n",
            "Requirement already satisfied: PyYAML<5.4,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli) (3.13)\n",
            "Collecting colorama<0.4.4,>=0.2.5\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.16.3->awscli) (0.9.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.16.3->awscli) (2.8.1)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /usr/local/lib/python3.6/dist-packages (from botocore==1.16.3->awscli) (1.24.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli) (0.4.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.16.3->awscli) (1.12.0)\n",
            "Building wheels for collected packages: awscli\n",
            "  Building wheel for awscli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for awscli: filename=awscli-1.18.53-py2.py3-none-any.whl size=3024943 sha256=6090c9c6751ca6aa0a1d59aa25e4b5f7893bd1a4ab9bb31aceaf11d9d9697137\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/95/d8/a77e2e0ae2f714211c4a8db3672b9392daedc9dfed6bac05c2\n",
            "Successfully built awscli\n",
            "Installing collected packages: botocore, rsa, colorama, awscli\n",
            "  Found existing installation: botocore 1.16.1\n",
            "    Uninstalling botocore-1.16.1:\n",
            "      Successfully uninstalled botocore-1.16.1\n",
            "  Found existing installation: rsa 4.0\n",
            "    Uninstalling rsa-4.0:\n",
            "      Successfully uninstalled rsa-4.0\n",
            "Successfully installed awscli-1.18.53 botocore-1.16.3 colorama-0.4.3 rsa-3.4.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "rsa"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Completed 7 Bytes/2.3 GiB (13 Bytes/s) with 10 file(s) remaining\rdownload: s3://models.dobro.ai/gpt2/ru/unfreeze_all/m_checkpoint-3364613/step.txt to gpt2/m_checkpoint-3364613/step.txt\n",
            "Completed 7 Bytes/2.3 GiB (13 Bytes/s) with 9 file(s) remaining\rCompleted 681 Bytes/2.3 GiB (1.1 KiB/s) with 9 file(s) remaining\rdownload: s3://models.dobro.ai/gpt2/ru/unfreeze_all/m_checkpoint-3364613/config.json to gpt2/m_checkpoint-3364613/config.json\n",
            "Completed 681 Bytes/2.3 GiB (1.1 KiB/s) with 8 file(s) remaining\rCompleted 2.0 KiB/2.3 GiB (3.5 KiB/s) with 8 file(s) remaining  \rCompleted 2.6 KiB/2.3 GiB (4.5 KiB/s) with 8 file(s) remaining  \rdownload: s3://models.dobro.ai/gpt2/ru/unfreeze_all/m_checkpoint-3364613/training_args.bin to gpt2/m_checkpoint-3364613/training_args.bin\n",
            "Completed 2.6 KiB/2.3 GiB (4.5 KiB/s) with 7 file(s) remaining\rdownload: s3://models.dobro.ai/gpt2/ru/unfreeze_all/s_checkpoint-1900000/config.json to gpt2/s_checkpoint-1900000/config.json\n",
            "download: s3://models.dobro.ai/gpt2/ru/unfreeze_all/s_checkpoint-1900000/training_args.bin to gpt2/s_checkpoint-1900000/training_args.bin\n",
            "download: s3://models.dobro.ai/gpt2/ru/unfreeze_all/s_checkpoint-1900000/encoder.model to gpt2/s_checkpoint-1900000/encoder.model\n",
            "download: s3://models.dobro.ai/gpt2/ru/unfreeze_all/m_checkpoint-3364613/encoder.model to gpt2/m_checkpoint-3364613/encoder.model\n",
            "download: s3://models.dobro.ai/gpt2/ru/unfreeze_all/s_checkpoint-1900000/step.txt to gpt2/s_checkpoint-1900000/step.txt\n",
            "download: s3://models.dobro.ai/gpt2/ru/unfreeze_all/s_checkpoint-1900000/pytorch_model.bin to gpt2/s_checkpoint-1900000/pytorch_model.bin\n",
            "download: s3://models.dobro.ai/gpt2/ru/unfreeze_all/m_checkpoint-3364613/pytorch_model.bin to gpt2/m_checkpoint-3364613/pytorch_model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lICQFj-Fn4uZ",
        "colab_type": "code",
        "outputId": "8e3330e8-228f-43d9-872c-43574a1ed741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        }
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 19.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.13.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 31.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.16.3)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.1->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=08a8790abd9313c53db05bc1d878b27876163b470ce3380eb219d81bbd670e8f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZP-yi_yBX-O2",
        "colab_type": "code",
        "outputId": "546908cd-3c45-4513-c314-9c3c4a598f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "cd apex\n",
        "pip install -v --no-cache-dir ./"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFlCwY4BZgcO",
        "colab_type": "code",
        "outputId": "e4a88c3a-b010-4d9e-ae71-6173c0284b6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/116)\u001b[K\rremote: Counting objects:   1% (2/116)\u001b[K\rremote: Counting objects:   2% (3/116)\u001b[K\rremote: Counting objects:   3% (4/116)\u001b[K\rremote: Counting objects:   4% (5/116)\u001b[K\rremote: Counting objects:   5% (6/116)\u001b[K\rremote: Counting objects:   6% (7/116)\u001b[K\rremote: Counting objects:   7% (9/116)\u001b[K\rremote: Counting objects:   8% (10/116)\u001b[K\rremote: Counting objects:   9% (11/116)\u001b[K\rremote: Counting objects:  10% (12/116)\u001b[K\rremote: Counting objects:  11% (13/116)\u001b[K\rremote: Counting objects:  12% (14/116)\u001b[K\rremote: Counting objects:  13% (16/116)\u001b[K\rremote: Counting objects:  14% (17/116)\u001b[K\rremote: Counting objects:  15% (18/116)\u001b[K\rremote: Counting objects:  16% (19/116)\u001b[K\rremote: Counting objects:  17% (20/116)\u001b[K\rremote: Counting objects:  18% (21/116)\u001b[K\rremote: Counting objects:  19% (23/116)\u001b[K\rremote: Counting objects:  20% (24/116)\u001b[K\rremote: Counting objects:  21% (25/116)\u001b[K\rremote: Counting objects:  22% (26/116)\u001b[K\rremote: Counting objects:  23% (27/116)\u001b[K\rremote: Counting objects:  24% (28/116)\u001b[K\rremote: Counting objects:  25% (29/116)\u001b[K\rremote: Counting objects:  26% (31/116)\u001b[K\rremote: Counting objects:  27% (32/116)\u001b[K\rremote: Counting objects:  28% (33/116)\u001b[K\rremote: Counting objects:  29% (34/116)\u001b[K\rremote: Counting objects:  30% (35/116)\u001b[K\rremote: Counting objects:  31% (36/116)\u001b[K\rremote: Counting objects:  32% (38/116)\u001b[K\rremote: Counting objects:  33% (39/116)\u001b[K\rremote: Counting objects:  34% (40/116)\u001b[K\rremote: Counting objects:  35% (41/116)\u001b[K\rremote: Counting objects:  36% (42/116)\u001b[K\rremote: Counting objects:  37% (43/116)\u001b[K\rremote: Counting objects:  38% (45/116)\u001b[K\rremote: Counting objects:  39% (46/116)\u001b[K\rremote: Counting objects:  40% (47/116)\u001b[K\rremote: Counting objects:  41% (48/116)\u001b[K\rremote: Counting objects:  42% (49/116)\u001b[K\rremote: Counting objects:  43% (50/116)\u001b[K\rremote: Counting objects:  44% (52/116)\u001b[K\rremote: Counting objects:  45% (53/116)\u001b[K\rremote: Counting objects:  46% (54/116)\u001b[K\rremote: Counting objects:  47% (55/116)\u001b[K\rremote: Counting objects:  48% (56/116)\u001b[K\rremote: Counting objects:  49% (57/116)\u001b[K\rremote: Counting objects:  50% (58/116)\u001b[K\rremote: Counting objects:  51% (60/116)\u001b[K\rremote: Counting objects:  52% (61/116)\u001b[K\rremote: Counting objects:  53% (62/116)\u001b[K\rremote: Counting objects:  54% (63/116)\u001b[K\rremote: Counting objects:  55% (64/116)\u001b[K\rremote: Counting objects:  56% (65/116)\u001b[K\rremote: Counting objects:  57% (67/116)\u001b[K\rremote: Counting objects:  58% (68/116)\u001b[K\rremote: Counting objects:  59% (69/116)\u001b[K\rremote: Counting objects:  60% (70/116)\u001b[K\rremote: Counting objects:  61% (71/116)\u001b[K\rremote: Counting objects:  62% (72/116)\u001b[K\rremote: Counting objects:  63% (74/116)\u001b[K\rremote: Counting objects:  64% (75/116)\u001b[K\rremote: Counting objects:  65% (76/116)\u001b[K\rremote: Counting objects:  66% (77/116)\u001b[K\rremote: Counting objects:  67% (78/116)\u001b[K\rremote: Counting objects:  68% (79/116)\u001b[K\rremote: Counting objects:  69% (81/116)\u001b[K\rremote: Counting objects:  70% (82/116)\u001b[K\rremote: Counting objects:  71% (83/116)\u001b[K\rremote: Counting objects:  72% (84/116)\u001b[K\rremote: Counting objects:  73% (85/116)\u001b[K\rremote: Counting objects:  74% (86/116)\u001b[K\rremote: Counting objects:  75% (87/116)\u001b[K\rremote: Counting objects:  76% (89/116)\u001b[K\rremote: Counting objects:  77% (90/116)\u001b[K\rremote: Counting objects:  78% (91/116)\u001b[K\rremote: Counting objects:  79% (92/116)\u001b[K\rremote: Counting objects:  80% (93/116)\u001b[K\rremote: Counting objects:  81% (94/116)\u001b[K\rremote: Counting objects:  82% (96/116)\u001b[K\rremote: Counting objects:  83% (97/116)\u001b[K\rremote: Counting objects:  84% (98/116)\u001b[K\rremote: Counting objects:  85% (99/116)\u001b[K\rremote: Counting objects:  86% (100/116)\u001b[K\rremote: Counting objects:  87% (101/116)\u001b[K\rremote: Counting objects:  88% (103/116)\u001b[K\rremote: Counting objects:  89% (104/116)\u001b[K\rremote: Counting objects:  90% (105/116)\u001b[K\rremote: Counting objects:  91% (106/116)\u001b[K\rremote: Counting objects:  92% (107/116)\u001b[K\rremote: Counting objects:  93% (108/116)\u001b[K\rremote: Counting objects:  94% (110/116)\u001b[K\rremote: Counting objects:  95% (111/116)\u001b[K\rremote: Counting objects:  96% (112/116)\u001b[K\rremote: Counting objects:  97% (113/116)\u001b[K\rremote: Counting objects:  98% (114/116)\u001b[K\rremote: Counting objects:  99% (115/116)\u001b[K\rremote: Counting objects: 100% (116/116)\u001b[K\rremote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 6712 (delta 74), reused 58 (delta 35), pack-reused 6596\u001b[K\n",
            "Receiving objects: 100% (6712/6712), 13.75 MiB | 24.41 MiB/s, done.\n",
            "Resolving deltas: 100% (4462/4462), done.\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-a4cpz9gv\n",
            "Created temporary directory: /tmp/pip-req-tracker-dsuuoggf\n",
            "Created requirements tracker '/tmp/pip-req-tracker-dsuuoggf'\n",
            "Created temporary directory: /tmp/pip-install-j8pi5gcr\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-odnbz20h\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-dsuuoggf'\n",
            "    Running setup.py (path:/tmp/pip-req-build-odnbz20h/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "    torch.__version__  =  1.5.0+cu101\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-odnbz20h/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-odnbz20h/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-odnbz20h/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-odnbz20h/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-odnbz20h/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-odnbz20h/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-odnbz20h/setup.py:46: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-odnbz20h has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-dsuuoggf'\n",
            "Building wheels for collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-wheel-s7zkh75s\n",
            "  Building wheel for apex (setup.py) ... \u001b[?25l  Destination directory: /tmp/pip-wheel-s7zkh75s\n",
            "  Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-odnbz20h/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-odnbz20h/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d /tmp/pip-wheel-s7zkh75s --python-tag cp36\n",
            "  torch.__version__  =  1.5.0+cu101\n",
            "  /tmp/pip-req-build-odnbz20h/setup.py:46: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "    warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/apex\n",
            "  copying apex/__init__.py -> build/lib/apex\n",
            "  creating build/lib/apex/mlp\n",
            "  copying apex/mlp/mlp.py -> build/lib/apex/mlp\n",
            "  copying apex/mlp/__init__.py -> build/lib/apex/mlp\n",
            "  creating build/lib/apex/RNN\n",
            "  copying apex/RNN/cells.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/models.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/__init__.py -> build/lib/apex/RNN\n",
            "  copying apex/RNN/RNNBackend.py -> build/lib/apex/RNN\n",
            "  creating build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib/apex/multi_tensor_apply\n",
            "  copying apex/multi_tensor_apply/__init__.py -> build/lib/apex/multi_tensor_apply\n",
            "  creating build/lib/apex/amp\n",
            "  copying apex/amp/_process_optimizer.py -> build/lib/apex/amp\n",
            "  copying apex/amp/frontend.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_initialize.py -> build/lib/apex/amp\n",
            "  copying apex/amp/wrap.py -> build/lib/apex/amp\n",
            "  copying apex/amp/utils.py -> build/lib/apex/amp\n",
            "  copying apex/amp/rnn_compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__init__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/compat.py -> build/lib/apex/amp\n",
            "  copying apex/amp/_amp_state.py -> build/lib/apex/amp\n",
            "  copying apex/amp/handle.py -> build/lib/apex/amp\n",
            "  copying apex/amp/__version__.py -> build/lib/apex/amp\n",
            "  copying apex/amp/opt.py -> build/lib/apex/amp\n",
            "  copying apex/amp/amp.py -> build/lib/apex/amp\n",
            "  copying apex/amp/scaler.py -> build/lib/apex/amp\n",
            "  creating build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_adam.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_novograd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_sgd.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/fused_lamb.py -> build/lib/apex/optimizers\n",
            "  copying apex/optimizers/__init__.py -> build/lib/apex/optimizers\n",
            "  creating build/lib/apex/pyprof\n",
            "  copying apex/pyprof/__init__.py -> build/lib/apex/pyprof\n",
            "  creating build/lib/apex/parallel\n",
            "  copying apex/parallel/LARC.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/multiproc.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/__init__.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/distributed.py -> build/lib/apex/parallel\n",
            "  copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib/apex/parallel\n",
            "  creating build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/loss_scaler.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/__init__.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16util.py -> build/lib/apex/fp16_utils\n",
            "  copying apex/fp16_utils/fp16_optimizer.py -> build/lib/apex/fp16_utils\n",
            "  creating build/lib/apex/contrib\n",
            "  copying apex/contrib/__init__.py -> build/lib/apex/contrib\n",
            "  creating build/lib/apex/normalization\n",
            "  copying apex/normalization/fused_layer_norm.py -> build/lib/apex/normalization\n",
            "  copying apex/normalization/__init__.py -> build/lib/apex/normalization\n",
            "  creating build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/weight_norm.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/reparameterization.py -> build/lib/apex/reparameterization\n",
            "  copying apex/reparameterization/__init__.py -> build/lib/apex/reparameterization\n",
            "  creating build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/functional_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/torch_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/tensor_overrides.py -> build/lib/apex/amp/lists\n",
            "  copying apex/amp/lists/__init__.py -> build/lib/apex/amp/lists\n",
            "  creating build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/optim.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/linear.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/activation.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/normalization.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/output.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/blas.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/reduction.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/utility.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pointwise.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/softmax.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/data.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/embedding.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/pooling.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/dropout.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/prof.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/loss.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/randomSample.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/recurrentCell.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/misc.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__main__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/__init__.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/usage.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/base.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/conv.py -> build/lib/apex/pyprof/prof\n",
            "  copying apex/pyprof/prof/convert.py -> build/lib/apex/pyprof/prof\n",
            "  creating build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/nvmarker.py -> build/lib/apex/pyprof/nvtx\n",
            "  copying apex/pyprof/nvtx/__init__.py -> build/lib/apex/pyprof/nvtx\n",
            "  creating build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/db.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/nvvp.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/parse.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__main__.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/kernel.py -> build/lib/apex/pyprof/parse\n",
            "  copying apex/pyprof/parse/__init__.py -> build/lib/apex/pyprof/parse\n",
            "  creating build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/__init__.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib/apex/contrib/multihead_attn\n",
            "  creating build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_adam.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_sgd.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fused_lamb.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/__init__.py -> build/lib/apex/contrib/optimizers\n",
            "  copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib/apex/contrib/optimizers\n",
            "  creating build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/__init__.py -> build/lib/apex/contrib/groupbn\n",
            "  copying apex/contrib/groupbn/batch_norm.py -> build/lib/apex/contrib/groupbn\n",
            "  creating build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib/apex/contrib/xentropy\n",
            "  copying apex/contrib/xentropy/__init__.py -> build/lib/apex/contrib/xentropy\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/mlp.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  copying build/lib/apex/mlp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/mlp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/cells.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/models.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/__init__.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  copying build/lib/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/wheel/apex/RNN\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  copying build/lib/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/wheel/apex/multi_tensor_apply\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/frontend.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_initialize.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/wrap.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/utils.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp/lists\n",
            "  copying build/lib/apex/amp/__init__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/compat.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/handle.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/__version__.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/opt.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/amp.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  copying build/lib/apex/amp/scaler.py -> build/bdist.linux-x86_64/wheel/apex/amp\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  copying build/lib/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  copying build/lib/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/prof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/nvtx\n",
            "  copying build/lib/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  copying build/lib/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/wheel/apex/pyprof/parse\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/LARC.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/__init__.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/distributed.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  copying build/lib/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/wheel/apex/parallel\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/fp16_utils\n",
            "  copying build/lib/apex/__init__.py -> build/bdist.linux-x86_64/wheel/apex\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  copying build/lib/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/wheel/apex/contrib/multihead_attn\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  copying build/lib/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/wheel/apex/contrib/optimizers\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/wheel/apex/contrib/groupbn\n",
            "  copying build/lib/apex/contrib/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  copying build/lib/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/wheel/apex/contrib/xentropy\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  copying build/lib/apex/normalization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/normalization\n",
            "  creating build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  copying build/lib/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/wheel/apex/reparameterization\n",
            "  running install_egg_info\n",
            "  running egg_info\n",
            "  creating apex.egg-info\n",
            "  writing apex.egg-info/PKG-INFO\n",
            "  writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "  writing top-level names to apex.egg-info/top_level.txt\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "  Copying apex.egg-info to build/bdist.linux-x86_64/wheel/apex-0.1-py3.6.egg-info\n",
            "  running install_scripts\n",
            "  adding license file \"LICENSE\" (matched pattern \"LICEN[CS]E*\")\n",
            "  creating build/bdist.linux-x86_64/wheel/apex-0.1.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-s7zkh75s/apex-0.1-cp36-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'apex/__init__.py'\n",
            "  adding 'apex/RNN/RNNBackend.py'\n",
            "  adding 'apex/RNN/__init__.py'\n",
            "  adding 'apex/RNN/cells.py'\n",
            "  adding 'apex/RNN/models.py'\n",
            "  adding 'apex/amp/__init__.py'\n",
            "  adding 'apex/amp/__version__.py'\n",
            "  adding 'apex/amp/_amp_state.py'\n",
            "  adding 'apex/amp/_initialize.py'\n",
            "  adding 'apex/amp/_process_optimizer.py'\n",
            "  adding 'apex/amp/amp.py'\n",
            "  adding 'apex/amp/compat.py'\n",
            "  adding 'apex/amp/frontend.py'\n",
            "  adding 'apex/amp/handle.py'\n",
            "  adding 'apex/amp/opt.py'\n",
            "  adding 'apex/amp/rnn_compat.py'\n",
            "  adding 'apex/amp/scaler.py'\n",
            "  adding 'apex/amp/utils.py'\n",
            "  adding 'apex/amp/wrap.py'\n",
            "  adding 'apex/amp/lists/__init__.py'\n",
            "  adding 'apex/amp/lists/functional_overrides.py'\n",
            "  adding 'apex/amp/lists/tensor_overrides.py'\n",
            "  adding 'apex/amp/lists/torch_overrides.py'\n",
            "  adding 'apex/contrib/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/__init__.py'\n",
            "  adding 'apex/contrib/groupbn/batch_norm.py'\n",
            "  adding 'apex/contrib/multihead_attn/__init__.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn.py'\n",
            "  adding 'apex/contrib/multihead_attn/self_multihead_attn_func.py'\n",
            "  adding 'apex/contrib/optimizers/__init__.py'\n",
            "  adding 'apex/contrib/optimizers/fp16_optimizer.py'\n",
            "  adding 'apex/contrib/optimizers/fused_adam.py'\n",
            "  adding 'apex/contrib/optimizers/fused_lamb.py'\n",
            "  adding 'apex/contrib/optimizers/fused_sgd.py'\n",
            "  adding 'apex/contrib/xentropy/__init__.py'\n",
            "  adding 'apex/contrib/xentropy/softmax_xentropy.py'\n",
            "  adding 'apex/fp16_utils/__init__.py'\n",
            "  adding 'apex/fp16_utils/fp16_optimizer.py'\n",
            "  adding 'apex/fp16_utils/fp16util.py'\n",
            "  adding 'apex/fp16_utils/loss_scaler.py'\n",
            "  adding 'apex/mlp/__init__.py'\n",
            "  adding 'apex/mlp/mlp.py'\n",
            "  adding 'apex/multi_tensor_apply/__init__.py'\n",
            "  adding 'apex/multi_tensor_apply/multi_tensor_apply.py'\n",
            "  adding 'apex/normalization/__init__.py'\n",
            "  adding 'apex/normalization/fused_layer_norm.py'\n",
            "  adding 'apex/optimizers/__init__.py'\n",
            "  adding 'apex/optimizers/fused_adam.py'\n",
            "  adding 'apex/optimizers/fused_lamb.py'\n",
            "  adding 'apex/optimizers/fused_novograd.py'\n",
            "  adding 'apex/optimizers/fused_sgd.py'\n",
            "  adding 'apex/parallel/LARC.py'\n",
            "  adding 'apex/parallel/__init__.py'\n",
            "  adding 'apex/parallel/distributed.py'\n",
            "  adding 'apex/parallel/multiproc.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm.py'\n",
            "  adding 'apex/parallel/optimized_sync_batchnorm_kernel.py'\n",
            "  adding 'apex/parallel/sync_batchnorm.py'\n",
            "  adding 'apex/parallel/sync_batchnorm_kernel.py'\n",
            "  adding 'apex/pyprof/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/__init__.py'\n",
            "  adding 'apex/pyprof/nvtx/nvmarker.py'\n",
            "  adding 'apex/pyprof/parse/__init__.py'\n",
            "  adding 'apex/pyprof/parse/__main__.py'\n",
            "  adding 'apex/pyprof/parse/db.py'\n",
            "  adding 'apex/pyprof/parse/kernel.py'\n",
            "  adding 'apex/pyprof/parse/nvvp.py'\n",
            "  adding 'apex/pyprof/parse/parse.py'\n",
            "  adding 'apex/pyprof/prof/__init__.py'\n",
            "  adding 'apex/pyprof/prof/__main__.py'\n",
            "  adding 'apex/pyprof/prof/activation.py'\n",
            "  adding 'apex/pyprof/prof/base.py'\n",
            "  adding 'apex/pyprof/prof/blas.py'\n",
            "  adding 'apex/pyprof/prof/conv.py'\n",
            "  adding 'apex/pyprof/prof/convert.py'\n",
            "  adding 'apex/pyprof/prof/data.py'\n",
            "  adding 'apex/pyprof/prof/dropout.py'\n",
            "  adding 'apex/pyprof/prof/embedding.py'\n",
            "  adding 'apex/pyprof/prof/index_slice_join_mutate.py'\n",
            "  adding 'apex/pyprof/prof/linear.py'\n",
            "  adding 'apex/pyprof/prof/loss.py'\n",
            "  adding 'apex/pyprof/prof/misc.py'\n",
            "  adding 'apex/pyprof/prof/normalization.py'\n",
            "  adding 'apex/pyprof/prof/optim.py'\n",
            "  adding 'apex/pyprof/prof/output.py'\n",
            "  adding 'apex/pyprof/prof/pointwise.py'\n",
            "  adding 'apex/pyprof/prof/pooling.py'\n",
            "  adding 'apex/pyprof/prof/prof.py'\n",
            "  adding 'apex/pyprof/prof/randomSample.py'\n",
            "  adding 'apex/pyprof/prof/recurrentCell.py'\n",
            "  adding 'apex/pyprof/prof/reduction.py'\n",
            "  adding 'apex/pyprof/prof/softmax.py'\n",
            "  adding 'apex/pyprof/prof/usage.py'\n",
            "  adding 'apex/pyprof/prof/utility.py'\n",
            "  adding 'apex/reparameterization/__init__.py'\n",
            "  adding 'apex/reparameterization/reparameterization.py'\n",
            "  adding 'apex/reparameterization/weight_norm.py'\n",
            "  adding 'apex-0.1.dist-info/LICENSE'\n",
            "  adding 'apex-0.1.dist-info/METADATA'\n",
            "  adding 'apex-0.1.dist-info/WHEEL'\n",
            "  adding 'apex-0.1.dist-info/top_level.txt'\n",
            "  adding 'apex-0.1.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "\u001b[?25hdone\n",
            "  Created wheel for apex: filename=apex-0.1-cp36-none-any.whl size=157282 sha256=d4e6326cc130b9c1f3fa6cc2b5d3173a7011c904c823e32a01269e83b587302a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-a4cpz9gv/wheels/b1/3a/aa/d84906eaab780ae580c7a5686a33bf2820d8590ac3b60d5967\n",
            "  Removing source in /tmp/pip-req-build-odnbz20h\n",
            "Successfully built apex\n",
            "Installing collected packages: apex\n",
            "\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-dsuuoggf'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-QowDzeSit5",
        "colab_type": "code",
        "outputId": "a5f54c9a-c2ff-43a6-cc25-f0d28f7dcbbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "! pip install youtokentome"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome) (7.1.2)\n",
            "Installing collected packages: youtokentome\n",
            "Successfully installed youtokentome-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KoFOAl_n--x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "# from tqdm import tqdm as tqdm_base\n",
        "# def tqdm(*args, **kwargs):\n",
        "#     if hasattr(tqdm_base, '_instances'):\n",
        "#         for instance in list(tqdm_base._instances):\n",
        "#             tqdm_base._decr_instances(instance)\n",
        "#     return tqdm_base(*args, **kwargs)\n",
        "\n",
        "from transformers import (WEIGHTS_NAME, AdamW, \n",
        "                          # WarmupLinearSchedule,\n",
        "                                  BertConfig, BertForMaskedLM, BertTokenizer,\n",
        "                                  GPT2Config, GPT2LMHeadModel, GPT2Tokenizer,\n",
        "                                  OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer,\n",
        "                                  RobertaConfig, RobertaForMaskedLM, RobertaTokenizer,\n",
        "                                  DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
        "\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whcB1oX_o5jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger.setLevel('INFO')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7LgWLPwoDgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
        "    'openai-gpt': (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
        "    'bert': (BertConfig, BertForMaskedLM, BertTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForMaskedLM, RobertaTokenizer),\n",
        "    'distilbert': (DistilBertConfig, DistilBertForMaskedLM, DistilBertTokenizer)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44-EysG53iHQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dict2obj(d):\n",
        "  if isinstance(d, list):\n",
        "    d = [dict2obj(x) for x in d]\n",
        "  if not isinstance(d, dict):\n",
        "    return d\n",
        "  class C(object):\n",
        "    pass\n",
        "  o = C()\n",
        "  for k in d:\n",
        "    o.__dict__[k] = dict2obj(d[k])\n",
        "  return o\n",
        "\n",
        "BLOCK_SIZE = 256\n",
        "\n",
        "parser = {}\n",
        "\n",
        "parser['train_data_file'] = './oxxxymiron_lyrics_end_text.txt'\n",
        "parser['input_dir'] = './gpt2/m_checkpoint-3364613'\n",
        "parser['output_dir'] = './textgenmodels'\n",
        "\n",
        "parser['eval_data_file'] = './oxxxymiron_lyrics_end_text.txt'\n",
        "parser['model_type'] = 'gpt2' # bert\n",
        "parser['model_name_or_path'] = 'gpt2-medium' # 'bert-base-cased'\n",
        "parser['mlm'] = False \n",
        "parser['mlm_probability'] = False\n",
        "\n",
        "parser['config_name'] = \"\"\n",
        "parser['tokenizer_name'] = \"\"\n",
        "parser['cache_dir'] = \"\"\n",
        "parser['block_size'] = BLOCK_SIZE\n",
        "parser['do_train'] = True\n",
        "parser['do_eval'] = True\n",
        "parser['evaluate_during_training'] = True\n",
        "parser['do_lower_case'] = True\n",
        "\n",
        "parser['per_gpu_train_batch_size'] = 2\n",
        "parser['per_gpu_eval_batch_size'] = 2\n",
        "parser['gradient_accumulation_steps'] = 10\n",
        "parser['learning_rate'] = 0.001 # 5e-5\n",
        "parser['weight_decay'] = 0.0\n",
        "parser['adam_epsilon'] = 1e-8\n",
        "parser['max_grad_norm'] = 1.0\n",
        "parser['num_train_epochs'] = 5.0\n",
        "parser['max_steps'] = -1\n",
        "parser['warmup_steps'] = 100\n",
        "\n",
        "parser['logging_steps'] = 50\n",
        "parser['save_steps'] = 50\n",
        "parser['save_total_limit'] = None\n",
        "parser['eval_all_checkpoints'] = True\n",
        "parser['no_cuda'] = False\n",
        "parser['overwrite_output_dir'] = True\n",
        "parser['overwrite_cache'] = True\n",
        "parser['seed'] = 42\n",
        "\n",
        "parser['fp16'] = True\n",
        "parser['fp16_opt_level'] = 'O1'\n",
        "parser['local_rank'] = -1\n",
        "parser['server_ip'] = \"\"\n",
        "parser['server_port'] = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37c1oL47oVsY",
        "colab_type": "text"
      },
      "source": [
        "# Data loading\n",
        "https://github.com/huggingface/transformers/blob/master/examples/run_lm_finetuning.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdxejtgroZnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer, file_path='train', block_size=BLOCK_SIZE):\n",
        "        assert os.path.isfile(file_path)\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(directory, 'cached_lm_' + str(block_size) + '_' + filename)\n",
        "\n",
        "        if os.path.exists(cached_features_file):\n",
        "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, 'rb') as handle:\n",
        "                self.examples = pickle.load(handle)\n",
        "        else:\n",
        "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
        "\n",
        "            self.examples = []\n",
        "            with open(file_path, encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "            # tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "            tokenized_text = tokenizer.encode(text)\n",
        "\n",
        "            # TODO FIX WARNINGS WHERE SPECIAL TOKENS AND GPT2 OUTPUT TOO MUCH\n",
        "            for i in range(0, len(tokenized_text)-block_size+1, block_size): # Truncate in block of block_size\n",
        "                if parser['model_type'] == 'gpt2':\n",
        "                    self.examples.append(tokenized_text[i:i+block_size])\n",
        "                else:\n",
        "                    self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i:i+block_size]))\n",
        "                \n",
        "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
        "            # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
        "            # can change this behavior by adding (model specific) padding.\n",
        "\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, 'wb') as handle:\n",
        "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky7c504apCGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_and_cache_examples(args, tokenizer, evaluate=False):\n",
        "    dataset = TextDataset(tokenizer, file_path=args.eval_data_file if evaluate else args.train_data_file, block_size=args.block_size)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def _rotate_checkpoints(args, checkpoint_prefix, use_mtime=False):\n",
        "    if not args.save_total_limit:\n",
        "        return\n",
        "    if args.save_total_limit <= 0:\n",
        "        return\n",
        "\n",
        "    # Check if we should delete older checkpoint(s)\n",
        "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, '{}-*'.format(checkpoint_prefix)))\n",
        "    if len(glob_checkpoints) <= args.save_total_limit:\n",
        "        return\n",
        "\n",
        "    ordering_and_checkpoint_path = []\n",
        "    for path in glob_checkpoints:\n",
        "        if use_mtime:\n",
        "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
        "        else:\n",
        "            regex_match = re.match('.*{}-([0-9]+)'.format(checkpoint_prefix), path)\n",
        "            if regex_match and regex_match.groups():\n",
        "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
        "\n",
        "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
        "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
        "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
        "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
        "    for checkpoint in checkpoints_to_be_deleted:\n",
        "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
        "        shutil.rmtree(checkpoint)\n",
        "\n",
        "\n",
        "def mask_tokens(inputs, tokenizer, args):\n",
        "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
        "    labels = inputs.clone()\n",
        "    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "    probability_matrix = torch.full(labels.shape, args.mlm_probability)\n",
        "    special_tokens_mask = [tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()]\n",
        "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    labels[~masked_indices] = -1  # We only compute loss on masked tokens\n",
        "\n",
        "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    # 10% of the time, we replace masked input tokens with random word\n",
        "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model, tokenizer):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    # scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps = -1)\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
        "                                                          output_device=args.local_rank,\n",
        "                                                          find_unused_parameters=True)\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                   args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
        "    set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
        "                    tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    checkpoint_prefix = 'checkpoint'\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, '{}-{}'.format(checkpoint_prefix, global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    torch.save(parser, os.path.join(output_dir, 'training_args.bin'))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)\n",
        "\n",
        "    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
        "        inputs = inputs.to(args.device)\n",
        "        labels = labels.to(args.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
        "            lm_loss = outputs[0]\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
        "\n",
        "    result = {\n",
        "        \"perplexity\": perplexity,\n",
        "        'eval_loss': eval_loss\n",
        "    }\n",
        "\n",
        "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1AhHCpyp1MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# args = parser.parse_args()\n",
        "args = dict2obj(parser)\n",
        "\n",
        "if args.model_type in [\"bert\", \"roberta\", \"distilbert\"] and not args.mlm:\n",
        "  raise ValueError(\"BERT and RoBERTa do not have LM heads but masked LM heads. They must be run using the --mlm \"\n",
        "                    \"flag (masked language modeling).\")\n",
        "if args.eval_data_file is None and args.do_eval:\n",
        "  raise ValueError(\"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "                    \"or remove the --do_eval argument.\")\n",
        "\n",
        "if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
        "  raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAOzVvOVqTSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup distant debugging if needed\n",
        "if args.server_ip and args.server_port:\n",
        "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "    import ptvsd\n",
        "    print(\"Waiting for debugger attach\")\n",
        "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "    ptvsd.wait_for_attach()\n",
        "\n",
        "# Setup CUDA, GPU & distributed training\n",
        "if args.local_rank == -1 or args.no_cuda:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "    args.n_gpu = torch.cuda.device_count()\n",
        "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "    device = torch.device(\"cuda\", args.local_rank)\n",
        "    torch.distributed.init_process_group(backend='nccl')\n",
        "    args.n_gpu = 1\n",
        "args.device = device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qx4FyooWSu4",
        "colab_type": "code",
        "outputId": "1f631264-8db8-4cc2-a6b6-607889d82d34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Setup logging\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "                args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
        "\n",
        "# Set seed\n",
        "set_seed(args)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/06/2020 19:11:16 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tgHKPySRBZF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Byte pair encoding utilities\"\"\"\n",
        "import os\n",
        "import youtokentome as yttm\n",
        "import hashlib\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "import shutil\n",
        "import regex as re\n",
        "from os.path import samefile"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRwvZZnYRB4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NEW_LINE = '<|n|>'\n",
        "\n",
        "class YTEncoder(PreTrainedTokenizer):\n",
        "    def_name = 'encoder.model'\n",
        "    def __init__(self, filename, *inputs, **kwargs):\n",
        "        super().__init__(*inputs, **kwargs)\n",
        "        self.max_len_single_sentence = BLOCK_SIZE # no default special tokens - you can update this value if you add special tokens\n",
        "        self.max_len_sentences_pair = BLOCK_SIZE # no default special tokens - you can update this value if you add special tokens\n",
        "\n",
        "        if os.path.isdir(filename): filename = os.path.join(filename, self.def_name)\n",
        "\n",
        "        self.bpe = yttm.BPE(filename)\n",
        "        self.hash = hashlib.sha512(open(filename, 'rb').read()).hexdigest()[:10]\n",
        "        self.filename = filename\n",
        "\n",
        "    def encode(self, text):\n",
        "        if text and text[0] != ' ': text = ' ' + text\n",
        "        text = re.sub(r'(?=[^ ])([\\W])([\\w])',r'\\g<1> \\g<2>',text)\n",
        "        text = text.replace('\\n', f' {NEW_LINE} ')\n",
        "\n",
        "        return self.bpe.encode([text], output_type=yttm.OutputType.ID)[0]\n",
        "\n",
        "\n",
        "    def decode(self, tokens): # I hate regexps\n",
        "        if not isinstance(tokens,list):\n",
        "            tokens = tokens.tolist()\n",
        "        result = self.bpe.decode(tokens)[0]\n",
        "        result = re.sub(r'( )?(<\\|n\\|>)( )?', r'\\n', result)\n",
        "        result = re.sub(r'([\\n(]) (\\w)',r'\\g<1>\\g<2>', result)\n",
        "        result = re.sub(r'(\\W)([«\"''\\n(]|^) (\\w)',r'\\g<1>\\g<2>\\g<3>', result)\n",
        "        result = re.sub(r'(\\w)- (\\w)',r'\\g<1>-\\g<2>', result)\n",
        "        return result\n",
        "\n",
        "    def tokenize(self, text, **kwargs):\n",
        "        return self.encode(text)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, *inputs, **kwargs):\n",
        "        return cls(*inputs, **kwargs)\n",
        "\n",
        "    def add_special_tokens_single_sentence(self, token_ids):\n",
        "        return token_ids\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        src = self.filename\n",
        "        dst = os.path.join(save_directory, self.def_name)\n",
        "        if src != dst:\n",
        "            shutil.copyfile(src, dst)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcdDevtBWVus",
        "colab_type": "code",
        "outputId": "e16e09a1-4e55-4e1e-a1d8-4cbeb121f1eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load pretrained model and tokenizer\n",
        "if args.local_rank not in [-1, 0]:\n",
        "    torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
        "\n",
        "model = model_class.from_pretrained(args.input_dir)\n",
        "tokenizer = YTEncoder.from_pretrained(args.input_dir)\n",
        "model.to(args.device)\n",
        "\n",
        "if args.block_size <= 0:\n",
        "    args.block_size = tokenizer.max_len_single_sentence  # Our input block size will be the max possible for the model\n",
        "args.block_size = min(args.block_size, tokenizer.max_len_single_sentence)\n",
        "\n",
        "if args.local_rank == 0:\n",
        "    torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "logger.info(\"Training/evaluation parameters %s\", args)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/06/2020 19:11:16 - INFO - transformers.configuration_utils -   loading configuration file ./gpt2/m_checkpoint-3364613/config.json\n",
            "05/06/2020 19:11:16 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": null,\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "05/06/2020 19:11:16 - INFO - transformers.modeling_utils -   loading weights file ./gpt2/m_checkpoint-3364613/pytorch_model.bin\n",
            "05/06/2020 19:11:47 - INFO - __main__ -   Training/evaluation parameters <__main__.dict2obj.<locals>.C object at 0x7f743348a550>\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv39Fsh-ZshR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "        Args:\n",
        "            logits: logits distribution shape (batch size x vocabulary size)\n",
        "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
        "    if top_k > 0:\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "\n",
        "def sample_sequence(model, length, context, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
        "                    is_xlnet=False, is_xlm_mlm=False, xlm_mask_token=None, xlm_lang=None, device='cuda'):\n",
        "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "    with torch.no_grad():\n",
        "        for _ in trange(length):\n",
        "\n",
        "            inputs = {'input_ids': generated}\n",
        "            if is_xlnet: \n",
        "                # XLNet is a direct (predict same token, not next token) and bi-directional model by default\n",
        "                # => need one additional dummy token in the input (will be masked), attention mask and target mapping (see model docstring)\n",
        "                input_ids = torch.cat((generated, torch.zeros((1, 1), dtype=torch.long, device=device)), dim=1)\n",
        "                perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float, device=device)\n",
        "                perm_mask[:, :, -1] = 1.0  # Previous tokens don't see last token\n",
        "                target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float, device=device)\n",
        "                target_mapping[0, 0, -1] = 1.0  # predict last token\n",
        "                inputs = {'input_ids': input_ids, 'perm_mask': perm_mask, 'target_mapping': target_mapping}\n",
        "\n",
        "            if is_xlm_mlm and xlm_mask_token:\n",
        "                # XLM MLM models are direct models (predict same token, not next token)\n",
        "                # => need one additional dummy token in the input (will be masked and guessed)\n",
        "                input_ids = torch.cat((generated, torch.full((1, 1), xlm_mask_token, dtype=torch.long, device=device)), dim=1)\n",
        "                inputs = {'input_ids': input_ids}\n",
        "\n",
        "            if xlm_lang is not None:\n",
        "                inputs[\"langs\"] = torch.tensor([xlm_lang] * inputs[\"input_ids\"].shape[1], device=device).view(1, -1)\n",
        "\n",
        "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
        "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
        "\n",
        "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
        "            for i in range(num_samples):\n",
        "                for _ in set(generated[i].tolist()):\n",
        "                    next_token_logits[i, _] /= repetition_penalty\n",
        "                \n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            if temperature == 0: # greedy sampling:\n",
        "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
        "            else:\n",
        "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "    return generated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1rM-f2YZVtb",
        "colab_type": "code",
        "outputId": "f398df50-0111-4a33-b6ab-56b5ae8df1a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        }
      },
      "source": [
        "CONTEXT_TEXT = 'Всё ещё бабла нет, всё ещё с долгами канитель \\n \\\n",
        "Все ещё в подвале, всё ещё Parliament на бите \\n \\\n",
        "И я вернусь на трек, твой хуй как Тулуз-Лотрек \\n \\\n",
        "Если русский рэп в гробу сто лет, то я ебу скелет \\n \\\n",
        "И я построил альбом на костях \\n \\\n",
        "Этому не видно конца, как будто он голый толстяк \\n \\\n",
        "Каждый просит фит, каждый пишет: «Денег дам» \\n \\\n",
        "Вас миллион, но мой кумир – Гриша Перельман \\n'\n",
        "\n",
        "START_TEXT = ''\n",
        "CONTEXT_TEXT += START_TEXT\n",
        "\n",
        "context_tokens = tokenizer.encode(CONTEXT_TEXT)\n",
        "sampled = sample_sequence(model, \n",
        "                          100, \n",
        "                          context_tokens, \n",
        "                          temperature = 1.0,\n",
        "                          top_p=0.99\n",
        "                          )\n",
        "\n",
        "out = sampled[:, len(context_tokens):].tolist()\n",
        "text = ''.join([tokenizer.decode(o) for o in out])\n",
        "text = text[: text.find('<|endoftext|>')].split('\\n')\n",
        "\n",
        "print('-' * 20)\n",
        "\n",
        "for i, t in enumerate(text):\n",
        "    if i == 0:\n",
        "        print(START_TEXT + t)\n",
        "    else:\n",
        "        print(t)\n",
        "\n",
        "evaluate(args, model, tokenizer)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:04<00:00, 23.82it/s]\n",
            "05/06/2020 19:11:51 - INFO - __main__ -   Creating features from dataset file at .\n",
            "05/06/2020 19:11:51 - INFO - __main__ -   Saving features into cached file ./cached_lm_256_oxxxymiron_lyrics_end_text.txt\n",
            "05/06/2020 19:11:51 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "05/06/2020 19:11:51 - INFO - __main__ -     Num examples = 240\n",
            "05/06/2020 19:11:51 - INFO - __main__ -     Batch size = 2\n",
            "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "Линус Пончик и сам Швыдкий на пяток\n",
            "Я петь стал, я петь начал\n",
            "Я послушать вас больше не буду\n",
            "У меня свой голос, свой брил.\n",
            "За мерить – «на хера нам спа– бв-р»?\n",
            "Ништяк, стройте здания\n",
            "Дочь, клянусь! Зубрить неудобно,   так зачем же\n",
            "Не проще сходить в МАРТИ?!\n",
            "\n",
            "<UNK>ou Shall Execute\n",
            "\n",
            "<UNK>ou Shal\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 120/120 [00:09<00:00, 13.27it/s]\n",
            "05/06/2020 19:12:00 - INFO - __main__ -   ***** Eval results  *****\n",
            "05/06/2020 19:12:00 - INFO - __main__ -     eval_loss = 4.464891809225082\n",
            "05/06/2020 19:12:00 - INFO - __main__ -     perplexity = tensor(86.9116)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 4.464891809225082, 'perplexity': tensor(86.9116)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anYKS2-8WZnw",
        "colab_type": "code",
        "outputId": "98240519-4555-4401-e769-8879c48ae8ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training\n",
        "if args.do_train:\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "    train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/06/2020 19:12:00 - INFO - __main__ -   Loading features from cached file ./cached_lm_256_oxxxymiron_lyrics_end_text.txt\n",
            "05/06/2020 19:12:00 - INFO - __main__ -   ***** Running training *****\n",
            "05/06/2020 19:12:00 - INFO - __main__ -     Num examples = 240\n",
            "05/06/2020 19:12:00 - INFO - __main__ -     Num Epochs = 5\n",
            "05/06/2020 19:12:00 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
            "05/06/2020 19:12:00 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 20\n",
            "05/06/2020 19:12:00 - INFO - __main__ -     Gradient Accumulation steps = 10\n",
            "05/06/2020 19:12:00 - INFO - __main__ -     Total optimization steps = 60\n",
            "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\",)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration:   1%|          | 1/120 [00:00<00:33,  3.54it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 2/120 [00:00<00:33,  3.50it/s]\u001b[A\n",
            "Iteration:   2%|▎         | 3/120 [00:00<00:33,  3.48it/s]\u001b[A\n",
            "Iteration:   3%|▎         | 4/120 [00:01<00:33,  3.47it/s]\u001b[A\n",
            "Iteration:   4%|▍         | 5/120 [00:01<00:33,  3.45it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 6/120 [00:01<00:33,  3.43it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 7/120 [00:02<00:33,  3.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 8/120 [00:02<00:32,  3.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 9/120 [00:02<00:32,  3.41it/s]\u001b[A/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\n",
            "Iteration:   8%|▊         | 10/120 [00:03<00:35,  3.11it/s]\u001b[A\n",
            "Iteration:   9%|▉         | 11/120 [00:03<00:34,  3.18it/s]\u001b[A\n",
            "Iteration:  10%|█         | 12/120 [00:03<00:33,  3.25it/s]\u001b[A\n",
            "Iteration:  11%|█         | 13/120 [00:03<00:32,  3.30it/s]\u001b[A\n",
            "Iteration:  12%|█▏        | 14/120 [00:04<00:31,  3.32it/s]\u001b[A\n",
            "Iteration:  12%|█▎        | 15/120 [00:04<00:31,  3.35it/s]\u001b[A\n",
            "Iteration:  13%|█▎        | 16/120 [00:04<00:30,  3.37it/s]\u001b[A\n",
            "Iteration:  14%|█▍        | 17/120 [00:05<00:30,  3.36it/s]\u001b[A\n",
            "Iteration:  15%|█▌        | 18/120 [00:05<00:30,  3.37it/s]\u001b[A\n",
            "Iteration:  16%|█▌        | 19/120 [00:05<00:29,  3.39it/s]\u001b[A\n",
            "Iteration:  17%|█▋        | 20/120 [00:06<00:31,  3.18it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 21/120 [00:06<00:30,  3.21it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 22/120 [00:06<00:30,  3.26it/s]\u001b[A\n",
            "Iteration:  19%|█▉        | 23/120 [00:06<00:29,  3.31it/s]\u001b[A\n",
            "Iteration:  20%|██        | 24/120 [00:07<00:28,  3.32it/s]\u001b[A\n",
            "Iteration:  21%|██        | 25/120 [00:07<00:28,  3.35it/s]\u001b[A\n",
            "Iteration:  22%|██▏       | 26/120 [00:07<00:27,  3.38it/s]\u001b[A\n",
            "Iteration:  22%|██▎       | 27/120 [00:08<00:27,  3.35it/s]\u001b[A\n",
            "Iteration:  23%|██▎       | 28/120 [00:08<00:27,  3.36it/s]\u001b[A\n",
            "Iteration:  24%|██▍       | 29/120 [00:08<00:26,  3.38it/s]\u001b[A\n",
            "Iteration:  25%|██▌       | 30/120 [00:09<00:28,  3.17it/s]\u001b[A\n",
            "Iteration:  26%|██▌       | 31/120 [00:09<00:27,  3.22it/s]\u001b[A\n",
            "Iteration:  27%|██▋       | 32/120 [00:09<00:26,  3.27it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 33/120 [00:09<00:26,  3.29it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 34/120 [00:10<00:25,  3.31it/s]\u001b[A\n",
            "Iteration:  29%|██▉       | 35/120 [00:10<00:25,  3.34it/s]\u001b[A\n",
            "Iteration:  30%|███       | 36/120 [00:10<00:24,  3.37it/s]\u001b[A\n",
            "Iteration:  31%|███       | 37/120 [00:11<00:24,  3.40it/s]\u001b[A\n",
            "Iteration:  32%|███▏      | 38/120 [00:11<00:24,  3.37it/s]\u001b[A\n",
            "Iteration:  32%|███▎      | 39/120 [00:11<00:23,  3.39it/s]\u001b[A\n",
            "Iteration:  33%|███▎      | 40/120 [00:12<00:25,  3.17it/s]\u001b[A\n",
            "Iteration:  34%|███▍      | 41/120 [00:12<00:24,  3.24it/s]\u001b[A\n",
            "Iteration:  35%|███▌      | 42/120 [00:12<00:23,  3.29it/s]\u001b[A\n",
            "Iteration:  36%|███▌      | 43/120 [00:12<00:23,  3.34it/s]\u001b[A\n",
            "Iteration:  37%|███▋      | 44/120 [00:13<00:22,  3.33it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 45/120 [00:13<00:22,  3.37it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 46/120 [00:13<00:21,  3.39it/s]\u001b[A\n",
            "Iteration:  39%|███▉      | 47/120 [00:14<00:21,  3.40it/s]\u001b[A\n",
            "Iteration:  40%|████      | 48/120 [00:14<00:21,  3.41it/s]\u001b[A\n",
            "Iteration:  41%|████      | 49/120 [00:14<00:20,  3.41it/s]\u001b[A\n",
            "Iteration:  42%|████▏     | 50/120 [00:15<00:22,  3.18it/s]\u001b[A\n",
            "Iteration:  42%|████▎     | 51/120 [00:15<00:21,  3.25it/s]\u001b[A\n",
            "Iteration:  43%|████▎     | 52/120 [00:15<00:20,  3.31it/s]\u001b[A\n",
            "Iteration:  44%|████▍     | 53/120 [00:15<00:20,  3.32it/s]\u001b[A\n",
            "Iteration:  45%|████▌     | 54/120 [00:16<00:19,  3.32it/s]\u001b[A\n",
            "Iteration:  46%|████▌     | 55/120 [00:16<00:19,  3.34it/s]\u001b[A\n",
            "Iteration:  47%|████▋     | 56/120 [00:16<00:19,  3.34it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 57/120 [00:17<00:18,  3.36it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 58/120 [00:17<00:18,  3.38it/s]\u001b[A\n",
            "Iteration:  49%|████▉     | 59/120 [00:17<00:18,  3.38it/s]\u001b[A\n",
            "Iteration:  50%|█████     | 60/120 [00:18<00:18,  3.17it/s]\u001b[A\n",
            "Iteration:  51%|█████     | 61/120 [00:18<00:18,  3.22it/s]\u001b[A\n",
            "Iteration:  52%|█████▏    | 62/120 [00:18<00:17,  3.27it/s]\u001b[A\n",
            "Iteration:  52%|█████▎    | 63/120 [00:18<00:17,  3.32it/s]\u001b[A\n",
            "Iteration:  53%|█████▎    | 64/120 [00:19<00:16,  3.33it/s]\u001b[A\n",
            "Iteration:  54%|█████▍    | 65/120 [00:19<00:16,  3.35it/s]\u001b[A\n",
            "Iteration:  55%|█████▌    | 66/120 [00:19<00:16,  3.35it/s]\u001b[A\n",
            "Iteration:  56%|█████▌    | 67/120 [00:20<00:15,  3.37it/s]\u001b[A\n",
            "Iteration:  57%|█████▋    | 68/120 [00:20<00:15,  3.37it/s]\u001b[A\n",
            "Iteration:  57%|█████▊    | 69/120 [00:20<00:15,  3.39it/s]\u001b[A\n",
            "Iteration:  58%|█████▊    | 70/120 [00:21<00:15,  3.16it/s]\u001b[A\n",
            "Iteration:  59%|█████▉    | 71/120 [00:21<00:15,  3.21it/s]\u001b[A\n",
            "Iteration:  60%|██████    | 72/120 [00:21<00:14,  3.27it/s]\u001b[A\n",
            "Iteration:  61%|██████    | 73/120 [00:21<00:14,  3.32it/s]\u001b[A\n",
            "Iteration:  62%|██████▏   | 74/120 [00:22<00:13,  3.33it/s]\u001b[A\n",
            "Iteration:  62%|██████▎   | 75/120 [00:22<00:13,  3.35it/s]\u001b[A\n",
            "Iteration:  63%|██████▎   | 76/120 [00:22<00:13,  3.38it/s]\u001b[A\n",
            "Iteration:  64%|██████▍   | 77/120 [00:23<00:12,  3.38it/s]\u001b[A\n",
            "Iteration:  65%|██████▌   | 78/120 [00:23<00:12,  3.38it/s]\u001b[A\n",
            "Iteration:  66%|██████▌   | 79/120 [00:23<00:12,  3.38it/s]\u001b[A\n",
            "Iteration:  67%|██████▋   | 80/120 [00:24<00:12,  3.16it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 81/120 [00:24<00:12,  3.23it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 82/120 [00:24<00:11,  3.29it/s]\u001b[A\n",
            "Iteration:  69%|██████▉   | 83/120 [00:25<00:11,  3.32it/s]\u001b[A\n",
            "Iteration:  70%|███████   | 84/120 [00:25<00:10,  3.34it/s]\u001b[A\n",
            "Iteration:  71%|███████   | 85/120 [00:25<00:10,  3.36it/s]\u001b[A\n",
            "Iteration:  72%|███████▏  | 86/120 [00:25<00:10,  3.38it/s]\u001b[A\n",
            "Iteration:  72%|███████▎  | 87/120 [00:26<00:09,  3.38it/s]\u001b[A\n",
            "Iteration:  73%|███████▎  | 88/120 [00:26<00:09,  3.40it/s]\u001b[A\n",
            "Iteration:  74%|███████▍  | 89/120 [00:26<00:09,  3.38it/s]\u001b[A\n",
            "Iteration:  75%|███████▌  | 90/120 [00:27<00:10,  2.83it/s]\u001b[A\n",
            "Iteration:  76%|███████▌  | 91/120 [00:27<00:09,  2.97it/s]\u001b[A\n",
            "Iteration:  77%|███████▋  | 92/120 [00:27<00:09,  3.07it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 93/120 [00:28<00:08,  3.16it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 94/120 [00:28<00:08,  3.19it/s]\u001b[A\n",
            "Iteration:  79%|███████▉  | 95/120 [00:28<00:07,  3.26it/s]\u001b[A\n",
            "Iteration:  80%|████████  | 96/120 [00:29<00:07,  3.31it/s]\u001b[A\n",
            "Iteration:  81%|████████  | 97/120 [00:29<00:06,  3.34it/s]\u001b[A\n",
            "Iteration:  82%|████████▏ | 98/120 [00:29<00:06,  3.35it/s]\u001b[A\n",
            "Iteration:  82%|████████▎ | 99/120 [00:29<00:06,  3.38it/s]\u001b[A\n",
            "Iteration:  83%|████████▎ | 100/120 [00:30<00:06,  3.15it/s]\u001b[A\n",
            "Iteration:  84%|████████▍ | 101/120 [00:30<00:05,  3.22it/s]\u001b[A\n",
            "Iteration:  85%|████████▌ | 102/120 [00:30<00:05,  3.27it/s]\u001b[A\n",
            "Iteration:  86%|████████▌ | 103/120 [00:31<00:05,  3.32it/s]\u001b[A\n",
            "Iteration:  87%|████████▋ | 104/120 [00:31<00:04,  3.33it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 105/120 [00:31<00:04,  3.35it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 106/120 [00:32<00:04,  3.37it/s]\u001b[A\n",
            "Iteration:  89%|████████▉ | 107/120 [00:32<00:03,  3.35it/s]\u001b[A\n",
            "Iteration:  90%|█████████ | 108/120 [00:32<00:03,  3.36it/s]\u001b[A\n",
            "Iteration:  91%|█████████ | 109/120 [00:32<00:03,  3.36it/s]\u001b[A\n",
            "Iteration:  92%|█████████▏| 110/120 [00:33<00:03,  3.15it/s]\u001b[A\n",
            "Iteration:  92%|█████████▎| 111/120 [00:33<00:02,  3.21it/s]\u001b[A\n",
            "Iteration:  93%|█████████▎| 112/120 [00:33<00:02,  3.27it/s]\u001b[A\n",
            "Iteration:  94%|█████████▍| 113/120 [00:34<00:02,  3.31it/s]\u001b[A\n",
            "Iteration:  95%|█████████▌| 114/120 [00:34<00:01,  3.33it/s]\u001b[A\n",
            "Iteration:  96%|█████████▌| 115/120 [00:34<00:01,  3.34it/s]\u001b[A\n",
            "Iteration:  97%|█████████▋| 116/120 [00:35<00:01,  3.36it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 117/120 [00:35<00:00,  3.36it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 118/120 [00:35<00:00,  3.38it/s]\u001b[A\n",
            "Iteration:  99%|█████████▉| 119/120 [00:35<00:00,  3.39it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 120/120 [00:36<00:00,  3.30it/s]\n",
            "Epoch:  20%|██        | 1/5 [00:36<02:25, 36.34s/it]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1%|          | 1/120 [00:00<00:35,  3.40it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 2/120 [00:00<00:34,  3.40it/s]\u001b[A\n",
            "Iteration:   2%|▎         | 3/120 [00:00<00:34,  3.37it/s]\u001b[A\n",
            "Iteration:   3%|▎         | 4/120 [00:01<00:34,  3.38it/s]\u001b[A\n",
            "Iteration:   4%|▍         | 5/120 [00:01<00:34,  3.37it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 6/120 [00:01<00:33,  3.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 7/120 [00:02<00:33,  3.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 8/120 [00:02<00:33,  3.38it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 9/120 [00:02<00:33,  3.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 10/120 [00:03<00:35,  3.13it/s]\u001b[A\n",
            "Iteration:   9%|▉         | 11/120 [00:03<00:34,  3.20it/s]\u001b[A\n",
            "Iteration:  10%|█         | 12/120 [00:03<00:33,  3.26it/s]\u001b[A\n",
            "Iteration:  11%|█         | 13/120 [00:03<00:32,  3.30it/s]\u001b[A\n",
            "Iteration:  12%|█▏        | 14/120 [00:04<00:31,  3.33it/s]\u001b[A\n",
            "Iteration:  12%|█▎        | 15/120 [00:04<00:31,  3.34it/s]\u001b[A\n",
            "Iteration:  13%|█▎        | 16/120 [00:04<00:31,  3.34it/s]\u001b[A\n",
            "Iteration:  14%|█▍        | 17/120 [00:05<00:30,  3.35it/s]\u001b[A\n",
            "Iteration:  15%|█▌        | 18/120 [00:05<00:30,  3.37it/s]\u001b[A\n",
            "Iteration:  16%|█▌        | 19/120 [00:05<00:29,  3.37it/s]\u001b[A\n",
            "Iteration:  17%|█▋        | 20/120 [00:06<00:31,  3.15it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 21/120 [00:06<00:30,  3.22it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 22/120 [00:06<00:29,  3.27it/s]\u001b[A\n",
            "Iteration:  19%|█▉        | 23/120 [00:06<00:29,  3.32it/s]\u001b[A\n",
            "Iteration:  20%|██        | 24/120 [00:07<00:28,  3.35it/s]\u001b[A\n",
            "Iteration:  21%|██        | 25/120 [00:07<00:28,  3.36it/s]\u001b[A\n",
            "Iteration:  22%|██▏       | 26/120 [00:07<00:27,  3.37it/s]\u001b[A\n",
            "Iteration:  22%|██▎       | 27/120 [00:08<00:27,  3.38it/s]\u001b[A\n",
            "Iteration:  23%|██▎       | 28/120 [00:08<00:27,  3.39it/s]\u001b[A\n",
            "Iteration:  24%|██▍       | 29/120 [00:08<00:27,  3.37it/s]\u001b[A\n",
            "Iteration:  25%|██▌       | 30/120 [00:09<00:28,  3.15it/s]\u001b[A\n",
            "Iteration:  26%|██▌       | 31/120 [00:09<00:27,  3.22it/s]\u001b[A\n",
            "Iteration:  27%|██▋       | 32/120 [00:09<00:27,  3.25it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 33/120 [00:09<00:26,  3.28it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 34/120 [00:10<00:26,  3.31it/s]\u001b[A\n",
            "Iteration:  29%|██▉       | 35/120 [00:10<00:25,  3.33it/s]\u001b[A\n",
            "Iteration:  30%|███       | 36/120 [00:10<00:25,  3.35it/s]\u001b[A\n",
            "Iteration:  31%|███       | 37/120 [00:11<00:24,  3.36it/s]\u001b[A\n",
            "Iteration:  32%|███▏      | 38/120 [00:11<00:24,  3.37it/s]\u001b[A\n",
            "Iteration:  32%|███▎      | 39/120 [00:11<00:24,  3.36it/s]\u001b[A\n",
            "Iteration:  33%|███▎      | 40/120 [00:12<00:25,  3.15it/s]\u001b[A\n",
            "Iteration:  34%|███▍      | 41/120 [00:12<00:24,  3.20it/s]\u001b[A\n",
            "Iteration:  35%|███▌      | 42/120 [00:12<00:23,  3.26it/s]\u001b[A\n",
            "Iteration:  36%|███▌      | 43/120 [00:13<00:23,  3.29it/s]\u001b[A\n",
            "Iteration:  37%|███▋      | 44/120 [00:13<00:22,  3.31it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 45/120 [00:13<00:22,  3.34it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 46/120 [00:13<00:22,  3.35it/s]\u001b[A\n",
            "Iteration:  39%|███▉      | 47/120 [00:14<00:21,  3.36it/s]\u001b[A\n",
            "Iteration:  40%|████      | 48/120 [00:14<00:21,  3.38it/s]\u001b[A\n",
            "Iteration:  41%|████      | 49/120 [00:14<00:21,  3.36it/s]\u001b[A\n",
            "Iteration:  42%|████▏     | 50/120 [00:15<00:22,  3.14it/s]\u001b[A\n",
            "Iteration:  42%|████▎     | 51/120 [00:15<00:21,  3.20it/s]\u001b[A\n",
            "Iteration:  43%|████▎     | 52/120 [00:15<00:20,  3.26it/s]\u001b[A\n",
            "Iteration:  44%|████▍     | 53/120 [00:16<00:20,  3.30it/s]\u001b[A\n",
            "Iteration:  45%|████▌     | 54/120 [00:16<00:19,  3.33it/s]\u001b[A\n",
            "Iteration:  46%|████▌     | 55/120 [00:16<00:19,  3.35it/s]\u001b[A\n",
            "Iteration:  47%|████▋     | 56/120 [00:16<00:19,  3.36it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 57/120 [00:17<00:18,  3.38it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 58/120 [00:17<00:18,  3.39it/s]\u001b[A\n",
            "Iteration:  49%|████▉     | 59/120 [00:17<00:18,  3.37it/s]\u001b[A\n",
            "Iteration:  50%|█████     | 60/120 [00:18<00:18,  3.16it/s]\u001b[A\n",
            "Iteration:  51%|█████     | 61/120 [00:18<00:18,  3.22it/s]\u001b[A\n",
            "Iteration:  52%|█████▏    | 62/120 [00:18<00:17,  3.27it/s]\u001b[A\n",
            "Iteration:  52%|█████▎    | 63/120 [00:19<00:17,  3.30it/s]\u001b[A\n",
            "Iteration:  53%|█████▎    | 64/120 [00:19<00:16,  3.33it/s]\u001b[A\n",
            "Iteration:  54%|█████▍    | 65/120 [00:19<00:16,  3.35it/s]\u001b[A\n",
            "Iteration:  55%|█████▌    | 66/120 [00:19<00:16,  3.34it/s]\u001b[A\n",
            "Iteration:  56%|█████▌    | 67/120 [00:20<00:15,  3.36it/s]\u001b[A\n",
            "Iteration:  57%|█████▋    | 68/120 [00:20<00:15,  3.38it/s]\u001b[A\n",
            "Iteration:  57%|█████▊    | 69/120 [00:20<00:15,  3.36it/s]\u001b[A\n",
            "Iteration:  58%|█████▊    | 70/120 [00:21<00:15,  3.17it/s]\u001b[A\n",
            "Iteration:  59%|█████▉    | 71/120 [00:21<00:15,  3.22it/s]\u001b[A\n",
            "Iteration:  60%|██████    | 72/120 [00:21<00:14,  3.27it/s]\u001b[A\n",
            "Iteration:  61%|██████    | 73/120 [00:22<00:14,  3.31it/s]\u001b[A\n",
            "Iteration:  62%|██████▏   | 74/120 [00:22<00:13,  3.34it/s]\u001b[A\n",
            "Iteration:  62%|██████▎   | 75/120 [00:22<00:13,  3.35it/s]\u001b[A\n",
            "Iteration:  63%|██████▎   | 76/120 [00:22<00:13,  3.34it/s]\u001b[A\n",
            "Iteration:  64%|██████▍   | 77/120 [00:23<00:12,  3.36it/s]\u001b[A\n",
            "Iteration:  65%|██████▌   | 78/120 [00:23<00:12,  3.35it/s]\u001b[A\n",
            "Iteration:  66%|██████▌   | 79/120 [00:23<00:12,  3.36it/s]\u001b[A\n",
            "Iteration:  67%|██████▋   | 80/120 [00:24<00:12,  3.17it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 81/120 [00:24<00:12,  3.22it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 82/120 [00:24<00:11,  3.28it/s]\u001b[A\n",
            "Iteration:  69%|██████▉   | 83/120 [00:25<00:11,  3.32it/s]\u001b[A\n",
            "Iteration:  70%|███████   | 84/120 [00:25<00:10,  3.32it/s]\u001b[A\n",
            "Iteration:  71%|███████   | 85/120 [00:25<00:10,  3.31it/s]\u001b[A\n",
            "Iteration:  72%|███████▏  | 86/120 [00:26<00:10,  3.33it/s]\u001b[A\n",
            "Iteration:  72%|███████▎  | 87/120 [00:26<00:09,  3.31it/s]\u001b[A\n",
            "Iteration:  73%|███████▎  | 88/120 [00:26<00:09,  3.33it/s]\u001b[A\n",
            "Iteration:  74%|███████▍  | 89/120 [00:26<00:09,  3.35it/s]\u001b[A\n",
            "Iteration:  75%|███████▌  | 90/120 [00:27<00:09,  3.16it/s]\u001b[A\n",
            "Iteration:  76%|███████▌  | 91/120 [00:27<00:09,  3.20it/s]\u001b[A\n",
            "Iteration:  77%|███████▋  | 92/120 [00:27<00:08,  3.25it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 93/120 [00:28<00:08,  3.28it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 94/120 [00:28<00:07,  3.31it/s]\u001b[A\n",
            "Iteration:  79%|███████▉  | 95/120 [00:28<00:07,  3.34it/s]\u001b[A\n",
            "Iteration:  80%|████████  | 96/120 [00:29<00:07,  3.35it/s]\u001b[A\n",
            "Iteration:  81%|████████  | 97/120 [00:29<00:06,  3.36it/s]\u001b[A\n",
            "Iteration:  82%|████████▏ | 98/120 [00:29<00:06,  3.36it/s]\u001b[A\n",
            "Iteration:  82%|████████▎ | 99/120 [00:29<00:06,  3.38it/s]\u001b[A\n",
            "Iteration:  83%|████████▎ | 100/120 [00:30<00:06,  3.17it/s]\u001b[A\n",
            "Iteration:  84%|████████▍ | 101/120 [00:30<00:05,  3.24it/s]\u001b[A\n",
            "Iteration:  85%|████████▌ | 102/120 [00:30<00:05,  3.27it/s]\u001b[A\n",
            "Iteration:  86%|████████▌ | 103/120 [00:31<00:05,  3.27it/s]\u001b[A\n",
            "Iteration:  87%|████████▋ | 104/120 [00:31<00:04,  3.30it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 105/120 [00:31<00:04,  3.34it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 106/120 [00:32<00:04,  3.35it/s]\u001b[A\n",
            "Iteration:  89%|████████▉ | 107/120 [00:32<00:03,  3.37it/s]\u001b[A\n",
            "Iteration:  90%|█████████ | 108/120 [00:32<00:03,  3.37it/s]\u001b[A\n",
            "Iteration:  91%|█████████ | 109/120 [00:32<00:03,  3.39it/s]\u001b[A\n",
            "Iteration:  92%|█████████▏| 110/120 [00:33<00:03,  3.15it/s]\u001b[A\n",
            "Iteration:  92%|█████████▎| 111/120 [00:33<00:02,  3.22it/s]\u001b[A\n",
            "Iteration:  93%|█████████▎| 112/120 [00:33<00:02,  3.25it/s]\u001b[A\n",
            "Iteration:  94%|█████████▍| 113/120 [00:34<00:02,  3.30it/s]\u001b[A\n",
            "Iteration:  95%|█████████▌| 114/120 [00:34<00:01,  3.32it/s]\u001b[A\n",
            "Iteration:  96%|█████████▌| 115/120 [00:34<00:01,  3.34it/s]\u001b[A\n",
            "Iteration:  97%|█████████▋| 116/120 [00:35<00:01,  3.36it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 117/120 [00:35<00:00,  3.38it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 118/120 [00:35<00:00,  3.38it/s]\u001b[A\n",
            "Iteration:  99%|█████████▉| 119/120 [00:35<00:00,  3.39it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 120/120 [00:36<00:00,  3.30it/s]\n",
            "Epoch:  40%|████      | 2/5 [01:12<01:49, 36.34s/it]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1%|          | 1/120 [00:00<00:35,  3.37it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 2/120 [00:00<00:34,  3.38it/s]\u001b[A\n",
            "Iteration:   2%|▎         | 3/120 [00:00<00:34,  3.40it/s]\u001b[A\n",
            "Iteration:   3%|▎         | 4/120 [00:01<00:34,  3.40it/s]\u001b[A\n",
            "Iteration:   4%|▍         | 5/120 [00:01<00:33,  3.39it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 6/120 [00:01<00:33,  3.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 7/120 [00:02<00:33,  3.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 8/120 [00:02<00:32,  3.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 9/120 [00:02<00:32,  3.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 10/120 [00:03<00:34,  3.17it/s]\u001b[A\n",
            "Iteration:   9%|▉         | 11/120 [00:03<00:33,  3.22it/s]\u001b[A\n",
            "Iteration:  10%|█         | 12/120 [00:03<00:33,  3.27it/s]\u001b[A\n",
            "Iteration:  11%|█         | 13/120 [00:03<00:32,  3.30it/s]\u001b[A\n",
            "Iteration:  12%|█▏        | 14/120 [00:04<00:31,  3.34it/s]\u001b[A\n",
            "Iteration:  12%|█▎        | 15/120 [00:04<00:31,  3.36it/s]\u001b[A\n",
            "Iteration:  13%|█▎        | 16/120 [00:04<00:30,  3.37it/s]\u001b[A\n",
            "Iteration:  14%|█▍        | 17/120 [00:05<00:30,  3.35it/s]\u001b[A\n",
            "Iteration:  15%|█▌        | 18/120 [00:05<00:30,  3.37it/s]\u001b[A\n",
            "Iteration:  16%|█▌        | 19/120 [00:05<00:30,  3.36it/s]\u001b[A\n",
            "Iteration:  17%|█▋        | 20/120 [00:06<00:31,  3.17it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 21/120 [00:06<00:30,  3.21it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 22/120 [00:06<00:29,  3.27it/s]\u001b[A\n",
            "Iteration:  19%|█▉        | 23/120 [00:06<00:29,  3.31it/s]\u001b[A\n",
            "Iteration:  20%|██        | 24/120 [00:07<00:28,  3.35it/s]\u001b[A\n",
            "Iteration:  21%|██        | 25/120 [00:07<00:28,  3.36it/s]\u001b[A\n",
            "Iteration:  22%|██▏       | 26/120 [00:07<00:27,  3.37it/s]\u001b[A\n",
            "Iteration:  22%|██▎       | 27/120 [00:08<00:27,  3.34it/s]\u001b[A\n",
            "Iteration:  23%|██▎       | 28/120 [00:08<00:27,  3.36it/s]\u001b[A\n",
            "Iteration:  24%|██▍       | 29/120 [00:08<00:26,  3.38it/s]\u001b[A\n",
            "Iteration:  25%|██▌       | 30/120 [00:09<00:28,  3.17it/s]\u001b[A\n",
            "Iteration:  26%|██▌       | 31/120 [00:09<00:27,  3.23it/s]\u001b[A\n",
            "Iteration:  27%|██▋       | 32/120 [00:09<00:26,  3.29it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 33/120 [00:09<00:26,  3.32it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 34/120 [00:10<00:25,  3.35it/s]\u001b[A\n",
            "Iteration:  29%|██▉       | 35/120 [00:10<00:25,  3.36it/s]\u001b[A\n",
            "Iteration:  30%|███       | 36/120 [00:10<00:24,  3.39it/s]\u001b[A\n",
            "Iteration:  31%|███       | 37/120 [00:11<00:24,  3.37it/s]\u001b[A\n",
            "Iteration:  32%|███▏      | 38/120 [00:11<00:24,  3.39it/s]\u001b[A\n",
            "Iteration:  32%|███▎      | 39/120 [00:11<00:23,  3.40it/s]\u001b[A\n",
            "Iteration:  33%|███▎      | 40/120 [00:12<00:25,  3.18it/s]\u001b[A\n",
            "Iteration:  34%|███▍      | 41/120 [00:12<00:24,  3.24it/s]\u001b[A\n",
            "Iteration:  35%|███▌      | 42/120 [00:12<00:23,  3.28it/s]\u001b[A\n",
            "Iteration:  36%|███▌      | 43/120 [00:12<00:23,  3.31it/s]\u001b[A\n",
            "Iteration:  37%|███▋      | 44/120 [00:13<00:22,  3.34it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 45/120 [00:13<00:22,  3.34it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 46/120 [00:13<00:22,  3.36it/s]\u001b[A\n",
            "Iteration:  39%|███▉      | 47/120 [00:14<00:21,  3.38it/s]\u001b[A\n",
            "Iteration:  40%|████      | 48/120 [00:14<00:21,  3.39it/s]\u001b[A\n",
            "Iteration:  41%|████      | 49/120 [00:14<00:20,  3.39it/s]\u001b[A\n",
            "Iteration:  42%|████▏     | 50/120 [00:15<00:22,  3.17it/s]\u001b[A\n",
            "Iteration:  42%|████▎     | 51/120 [00:15<00:21,  3.24it/s]\u001b[A\n",
            "Iteration:  43%|████▎     | 52/120 [00:15<00:20,  3.29it/s]\u001b[A\n",
            "Iteration:  44%|████▍     | 53/120 [00:15<00:20,  3.33it/s]\u001b[A\n",
            "Iteration:  45%|████▌     | 54/120 [00:16<00:19,  3.35it/s]\u001b[A\n",
            "Iteration:  46%|████▌     | 55/120 [00:16<00:19,  3.35it/s]\u001b[A\n",
            "Iteration:  47%|████▋     | 56/120 [00:16<00:19,  3.36it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 57/120 [00:17<00:18,  3.37it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 58/120 [00:17<00:18,  3.39it/s]\u001b[A\n",
            "Iteration:  49%|████▉     | 59/120 [00:17<00:17,  3.40it/s]\u001b[A\n",
            "Iteration:  50%|█████     | 60/120 [00:18<00:19,  3.15it/s]\u001b[A\n",
            "Iteration:  51%|█████     | 61/120 [00:18<00:18,  3.22it/s]\u001b[A\n",
            "Iteration:  52%|█████▏    | 62/120 [00:18<00:17,  3.28it/s]\u001b[A\n",
            "Iteration:  52%|█████▎    | 63/120 [00:18<00:17,  3.32it/s]\u001b[A\n",
            "Iteration:  53%|█████▎    | 64/120 [00:19<00:16,  3.35it/s]\u001b[A\n",
            "Iteration:  54%|█████▍    | 65/120 [00:19<00:16,  3.37it/s]\u001b[A\n",
            "Iteration:  55%|█████▌    | 66/120 [00:19<00:15,  3.38it/s]\u001b[A\n",
            "Iteration:  56%|█████▌    | 67/120 [00:20<00:15,  3.40it/s]\u001b[A\n",
            "Iteration:  57%|█████▋    | 68/120 [00:20<00:15,  3.39it/s]\u001b[A\n",
            "Iteration:  57%|█████▊    | 69/120 [00:20<00:14,  3.40it/s]\u001b[A\n",
            "Iteration:  58%|█████▊    | 70/120 [00:21<00:15,  3.18it/s]\u001b[A\n",
            "Iteration:  59%|█████▉    | 71/120 [00:21<00:15,  3.24it/s]\u001b[A\n",
            "Iteration:  60%|██████    | 72/120 [00:21<00:14,  3.28it/s]\u001b[A\n",
            "Iteration:  61%|██████    | 73/120 [00:21<00:14,  3.32it/s]\u001b[A\n",
            "Iteration:  62%|██████▏   | 74/120 [00:22<00:13,  3.35it/s]\u001b[A\n",
            "Iteration:  62%|██████▎   | 75/120 [00:22<00:13,  3.38it/s]\u001b[A\n",
            "Iteration:  63%|██████▎   | 76/120 [00:22<00:13,  3.38it/s]\u001b[A\n",
            "Iteration:  64%|██████▍   | 77/120 [00:23<00:12,  3.37it/s]\u001b[A\n",
            "Iteration:  65%|██████▌   | 78/120 [00:23<00:12,  3.35it/s]\u001b[A\n",
            "Iteration:  66%|██████▌   | 79/120 [00:23<00:12,  3.36it/s]\u001b[A\n",
            "Iteration:  67%|██████▋   | 80/120 [00:24<00:12,  3.16it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 81/120 [00:24<00:12,  3.24it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 82/120 [00:24<00:11,  3.27it/s]\u001b[A\n",
            "Iteration:  69%|██████▉   | 83/120 [00:24<00:11,  3.31it/s]\u001b[A\n",
            "Iteration:  70%|███████   | 84/120 [00:25<00:10,  3.32it/s]\u001b[A\n",
            "Iteration:  71%|███████   | 85/120 [00:25<00:10,  3.36it/s]\u001b[A\n",
            "Iteration:  72%|███████▏  | 86/120 [00:25<00:10,  3.35it/s]\u001b[A\n",
            "Iteration:  72%|███████▎  | 87/120 [00:26<00:09,  3.37it/s]\u001b[A\n",
            "Iteration:  73%|███████▎  | 88/120 [00:26<00:09,  3.39it/s]\u001b[A\n",
            "Iteration:  74%|███████▍  | 89/120 [00:26<00:09,  3.40it/s]\u001b[A\n",
            "Iteration:  75%|███████▌  | 90/120 [00:27<00:09,  3.17it/s]\u001b[A\n",
            "Iteration:  76%|███████▌  | 91/120 [00:27<00:08,  3.23it/s]\u001b[A\n",
            "Iteration:  77%|███████▋  | 92/120 [00:27<00:08,  3.27it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 93/120 [00:28<00:08,  3.31it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 94/120 [00:28<00:07,  3.35it/s]\u001b[A\n",
            "Iteration:  79%|███████▉  | 95/120 [00:28<00:07,  3.36it/s]\u001b[A\n",
            "Iteration:  80%|████████  | 96/120 [00:28<00:07,  3.37it/s]\u001b[A\n",
            "Iteration:  81%|████████  | 97/120 [00:29<00:06,  3.40it/s]\u001b[A\n",
            "Iteration:  82%|████████▏ | 98/120 [00:29<00:06,  3.41it/s]\u001b[A\n",
            "Iteration:  82%|████████▎ | 99/120 [00:29<00:06,  3.41it/s]\u001b[A\n",
            "Iteration:  83%|████████▎ | 100/120 [00:30<00:06,  3.18it/s]\u001b[A\n",
            "Iteration:  84%|████████▍ | 101/120 [00:30<00:05,  3.25it/s]\u001b[A\n",
            "Iteration:  85%|████████▌ | 102/120 [00:30<00:05,  3.29it/s]\u001b[A\n",
            "Iteration:  86%|████████▌ | 103/120 [00:31<00:05,  3.33it/s]\u001b[A\n",
            "Iteration:  87%|████████▋ | 104/120 [00:31<00:04,  3.35it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 105/120 [00:31<00:04,  3.37it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 106/120 [00:31<00:04,  3.38it/s]\u001b[A\n",
            "Iteration:  89%|████████▉ | 107/120 [00:32<00:03,  3.39it/s]\u001b[A\n",
            "Iteration:  90%|█████████ | 108/120 [00:32<00:03,  3.40it/s]\u001b[A\n",
            "Iteration:  91%|█████████ | 109/120 [00:32<00:03,  3.38it/s]\u001b[A\n",
            "Iteration:  92%|█████████▏| 110/120 [00:33<00:03,  3.14it/s]\u001b[A\n",
            "Iteration:  92%|█████████▎| 111/120 [00:33<00:02,  3.20it/s]\u001b[A\n",
            "Iteration:  93%|█████████▎| 112/120 [00:33<00:02,  3.25it/s]\u001b[A\n",
            "Iteration:  94%|█████████▍| 113/120 [00:34<00:02,  3.29it/s]\u001b[A\n",
            "Iteration:  95%|█████████▌| 114/120 [00:34<00:01,  3.32it/s]\u001b[A\n",
            "Iteration:  96%|█████████▌| 115/120 [00:34<00:01,  3.35it/s]\u001b[A\n",
            "Iteration:  97%|█████████▋| 116/120 [00:34<00:01,  3.37it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 117/120 [00:35<00:00,  3.39it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 118/120 [00:35<00:00,  3.40it/s]\u001b[A\n",
            "Iteration:  99%|█████████▉| 119/120 [00:35<00:00,  3.39it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 120/120 [00:36<00:00,  3.32it/s]\n",
            "Epoch:  60%|██████    | 3/5 [01:48<01:12, 36.29s/it]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1%|          | 1/120 [00:00<00:35,  3.38it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 2/120 [00:00<00:35,  3.36it/s]\u001b[A\n",
            "Iteration:   2%|▎         | 3/120 [00:00<00:34,  3.38it/s]\u001b[A\n",
            "Iteration:   3%|▎         | 4/120 [00:01<00:34,  3.38it/s]\u001b[A\n",
            "Iteration:   4%|▍         | 5/120 [00:01<00:33,  3.39it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 6/120 [00:01<00:33,  3.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 7/120 [00:02<00:33,  3.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 8/120 [00:02<00:32,  3.39it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 9/120 [00:02<00:32,  3.37it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 10/120 [00:03<00:34,  3.16it/s]\u001b[A\n",
            "Iteration:   9%|▉         | 11/120 [00:03<00:33,  3.23it/s]\u001b[A\n",
            "Iteration:  10%|█         | 12/120 [00:03<00:33,  3.27it/s]\u001b[A\n",
            "Iteration:  11%|█         | 13/120 [00:03<00:32,  3.28it/s]\u001b[A\n",
            "Iteration:  12%|█▏        | 14/120 [00:04<00:32,  3.30it/s]\u001b[A\n",
            "Iteration:  12%|█▎        | 15/120 [00:04<00:31,  3.32it/s]\u001b[A\n",
            "Iteration:  13%|█▎        | 16/120 [00:04<00:31,  3.34it/s]\u001b[A\n",
            "Iteration:  14%|█▍        | 17/120 [00:05<00:30,  3.36it/s]\u001b[A\n",
            "Iteration:  15%|█▌        | 18/120 [00:05<00:30,  3.37it/s]\u001b[A\n",
            "Iteration:  16%|█▌        | 19/120 [00:05<00:30,  3.35it/s]\u001b[A\n",
            "Iteration:  17%|█▋        | 20/120 [00:06<00:31,  3.14it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 21/120 [00:06<00:30,  3.20it/s]\u001b[A\n",
            "Iteration:  18%|█▊        | 22/120 [00:06<00:30,  3.26it/s]\u001b[A\n",
            "Iteration:  19%|█▉        | 23/120 [00:06<00:29,  3.30it/s]\u001b[A\n",
            "Iteration:  20%|██        | 24/120 [00:07<00:28,  3.34it/s]\u001b[A\n",
            "Iteration:  21%|██        | 25/120 [00:07<00:28,  3.34it/s]\u001b[A\n",
            "Iteration:  22%|██▏       | 26/120 [00:07<00:27,  3.37it/s]\u001b[A\n",
            "Iteration:  22%|██▎       | 27/120 [00:08<00:27,  3.35it/s]\u001b[A\n",
            "Iteration:  23%|██▎       | 28/120 [00:08<00:27,  3.35it/s]\u001b[A\n",
            "Iteration:  24%|██▍       | 29/120 [00:08<00:27,  3.36it/s]\u001b[A\n",
            "Iteration:  25%|██▌       | 30/120 [00:09<00:28,  3.12it/s]\u001b[A\n",
            "Iteration:  26%|██▌       | 31/120 [00:09<00:27,  3.19it/s]\u001b[A\n",
            "Iteration:  27%|██▋       | 32/120 [00:09<00:27,  3.25it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 33/120 [00:09<00:26,  3.29it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 34/120 [00:10<00:25,  3.33it/s]\u001b[A\n",
            "Iteration:  29%|██▉       | 35/120 [00:10<00:25,  3.35it/s]\u001b[A\n",
            "Iteration:  30%|███       | 36/120 [00:10<00:25,  3.35it/s]\u001b[A\n",
            "Iteration:  31%|███       | 37/120 [00:11<00:24,  3.36it/s]\u001b[A\n",
            "Iteration:  32%|███▏      | 38/120 [00:11<00:24,  3.35it/s]\u001b[A\n",
            "Iteration:  32%|███▎      | 39/120 [00:11<00:24,  3.35it/s]\u001b[A\n",
            "Iteration:  33%|███▎      | 40/120 [00:12<00:25,  3.15it/s]\u001b[A\n",
            "Iteration:  34%|███▍      | 41/120 [00:12<00:24,  3.22it/s]\u001b[A\n",
            "Iteration:  35%|███▌      | 42/120 [00:12<00:23,  3.28it/s]\u001b[A\n",
            "Iteration:  36%|███▌      | 43/120 [00:13<00:23,  3.31it/s]\u001b[A\n",
            "Iteration:  37%|███▋      | 44/120 [00:13<00:22,  3.31it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 45/120 [00:13<00:22,  3.33it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 46/120 [00:13<00:22,  3.34it/s]\u001b[A\n",
            "Iteration:  39%|███▉      | 47/120 [00:14<00:21,  3.37it/s]\u001b[A\n",
            "Iteration:  40%|████      | 48/120 [00:14<00:21,  3.38it/s]\u001b[A\n",
            "Iteration:  41%|████      | 49/120 [00:14<00:21,  3.37it/s]\u001b[A\n",
            "Iteration:  42%|████▏     | 50/120 [00:15<00:22,  3.12it/s]\u001b[A\n",
            "Iteration:  42%|████▎     | 51/120 [00:15<00:21,  3.21it/s]\u001b[A\n",
            "Iteration:  43%|████▎     | 52/120 [00:15<00:20,  3.27it/s]\u001b[A\n",
            "Iteration:  44%|████▍     | 53/120 [00:16<00:20,  3.31it/s]\u001b[A\n",
            "Iteration:  45%|████▌     | 54/120 [00:16<00:19,  3.33it/s]\u001b[A\n",
            "Iteration:  46%|████▌     | 55/120 [00:16<00:19,  3.35it/s]\u001b[A\n",
            "Iteration:  47%|████▋     | 56/120 [00:16<00:18,  3.38it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 57/120 [00:17<00:18,  3.37it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 58/120 [00:17<00:18,  3.37it/s]\u001b[A\n",
            "Iteration:  49%|████▉     | 59/120 [00:17<00:18,  3.37it/s]\u001b[A\n",
            "Iteration:  50%|█████     | 60/120 [00:18<00:18,  3.16it/s]\u001b[A\n",
            "Iteration:  51%|█████     | 61/120 [00:18<00:18,  3.23it/s]\u001b[A\n",
            "Iteration:  52%|█████▏    | 62/120 [00:18<00:17,  3.28it/s]\u001b[A\n",
            "Iteration:  52%|█████▎    | 63/120 [00:19<00:17,  3.31it/s]\u001b[A\n",
            "Iteration:  53%|█████▎    | 64/120 [00:19<00:16,  3.35it/s]\u001b[A\n",
            "Iteration:  54%|█████▍    | 65/120 [00:19<00:16,  3.35it/s]\u001b[A\n",
            "Iteration:  55%|█████▌    | 66/120 [00:19<00:16,  3.35it/s]\u001b[A\n",
            "Iteration:  56%|█████▌    | 67/120 [00:20<00:15,  3.37it/s]\u001b[A\n",
            "Iteration:  57%|█████▋    | 68/120 [00:20<00:15,  3.37it/s]\u001b[A\n",
            "Iteration:  57%|█████▊    | 69/120 [00:20<00:15,  3.38it/s]\u001b[A\n",
            "Iteration:  58%|█████▊    | 70/120 [00:21<00:15,  3.16it/s]\u001b[A\n",
            "Iteration:  59%|█████▉    | 71/120 [00:21<00:15,  3.22it/s]\u001b[A\n",
            "Iteration:  60%|██████    | 72/120 [00:21<00:14,  3.26it/s]\u001b[A\n",
            "Iteration:  61%|██████    | 73/120 [00:22<00:14,  3.26it/s]\u001b[A\n",
            "Iteration:  62%|██████▏   | 74/120 [00:22<00:14,  3.27it/s]\u001b[A\n",
            "Iteration:  62%|██████▎   | 75/120 [00:22<00:13,  3.31it/s]\u001b[A\n",
            "Iteration:  63%|██████▎   | 76/120 [00:22<00:13,  3.33it/s]\u001b[A\n",
            "Iteration:  64%|██████▍   | 77/120 [00:23<00:12,  3.35it/s]\u001b[A\n",
            "Iteration:  65%|██████▌   | 78/120 [00:23<00:12,  3.36it/s]\u001b[A\n",
            "Iteration:  66%|██████▌   | 79/120 [00:23<00:12,  3.38it/s]\u001b[A\n",
            "Iteration:  67%|██████▋   | 80/120 [00:24<00:12,  3.11it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 81/120 [00:24<00:12,  3.19it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 82/120 [00:24<00:11,  3.26it/s]\u001b[A\n",
            "Iteration:  69%|██████▉   | 83/120 [00:25<00:11,  3.27it/s]\u001b[A\n",
            "Iteration:  70%|███████   | 84/120 [00:25<00:10,  3.30it/s]\u001b[A\n",
            "Iteration:  71%|███████   | 85/120 [00:25<00:10,  3.34it/s]\u001b[A\n",
            "Iteration:  72%|███████▏  | 86/120 [00:26<00:10,  3.33it/s]\u001b[A\n",
            "Iteration:  72%|███████▎  | 87/120 [00:26<00:09,  3.36it/s]\u001b[A\n",
            "Iteration:  73%|███████▎  | 88/120 [00:26<00:09,  3.36it/s]\u001b[A\n",
            "Iteration:  74%|███████▍  | 89/120 [00:26<00:09,  3.35it/s]\u001b[A\n",
            "Iteration:  75%|███████▌  | 90/120 [00:27<00:09,  3.12it/s]\u001b[A\n",
            "Iteration:  76%|███████▌  | 91/120 [00:27<00:09,  3.19it/s]\u001b[A\n",
            "Iteration:  77%|███████▋  | 92/120 [00:27<00:08,  3.25it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 93/120 [00:28<00:08,  3.29it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 94/120 [00:28<00:07,  3.33it/s]\u001b[A\n",
            "Iteration:  79%|███████▉  | 95/120 [00:28<00:07,  3.35it/s]\u001b[A\n",
            "Iteration:  80%|████████  | 96/120 [00:29<00:07,  3.36it/s]\u001b[A\n",
            "Iteration:  81%|████████  | 97/120 [00:29<00:06,  3.37it/s]\u001b[A\n",
            "Iteration:  82%|████████▏ | 98/120 [00:29<00:06,  3.38it/s]\u001b[A\n",
            "Iteration:  82%|████████▎ | 99/120 [00:29<00:06,  3.40it/s]\u001b[A\n",
            "Iteration:  83%|████████▎ | 100/120 [00:30<00:06,  3.17it/s]\u001b[A\n",
            "Iteration:  84%|████████▍ | 101/120 [00:30<00:05,  3.21it/s]\u001b[A\n",
            "Iteration:  85%|████████▌ | 102/120 [00:30<00:05,  3.27it/s]\u001b[A\n",
            "Iteration:  86%|████████▌ | 103/120 [00:31<00:05,  3.30it/s]\u001b[A\n",
            "Iteration:  87%|████████▋ | 104/120 [00:31<00:04,  3.30it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 105/120 [00:31<00:04,  3.30it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 106/120 [00:32<00:04,  3.31it/s]\u001b[A\n",
            "Iteration:  89%|████████▉ | 107/120 [00:32<00:03,  3.34it/s]\u001b[A\n",
            "Iteration:  90%|█████████ | 108/120 [00:32<00:03,  3.36it/s]\u001b[A\n",
            "Iteration:  91%|█████████ | 109/120 [00:32<00:03,  3.38it/s]\u001b[A\n",
            "Iteration:  92%|█████████▏| 110/120 [00:33<00:03,  3.13it/s]\u001b[A\n",
            "Iteration:  92%|█████████▎| 111/120 [00:33<00:02,  3.20it/s]\u001b[A\n",
            "Iteration:  93%|█████████▎| 112/120 [00:33<00:02,  3.27it/s]\u001b[A\n",
            "Iteration:  94%|█████████▍| 113/120 [00:34<00:02,  3.30it/s]\u001b[A\n",
            "Iteration:  95%|█████████▌| 114/120 [00:34<00:01,  3.30it/s]\u001b[A\n",
            "Iteration:  96%|█████████▌| 115/120 [00:34<00:01,  3.34it/s]\u001b[A\n",
            "Iteration:  97%|█████████▋| 116/120 [00:35<00:01,  3.35it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 117/120 [00:35<00:00,  3.33it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 118/120 [00:35<00:00,  3.35it/s]\u001b[A\n",
            "Iteration:  99%|█████████▉| 119/120 [00:36<00:00,  3.37it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 120/120 [00:36<00:00,  3.30it/s]\n",
            "Epoch:  80%|████████  | 4/5 [02:25<00:36, 36.32s/it]\n",
            "Iteration:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1%|          | 1/120 [00:00<00:35,  3.31it/s]\u001b[A\n",
            "Iteration:   2%|▏         | 2/120 [00:00<00:35,  3.32it/s]\u001b[A\n",
            "Iteration:   2%|▎         | 3/120 [00:00<00:34,  3.34it/s]\u001b[A\n",
            "Iteration:   3%|▎         | 4/120 [00:01<00:34,  3.35it/s]\u001b[A\n",
            "Iteration:   4%|▍         | 5/120 [00:01<00:34,  3.37it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 6/120 [00:01<00:33,  3.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 7/120 [00:02<00:33,  3.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 8/120 [00:02<00:33,  3.36it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 9/120 [00:02<00:32,  3.38it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 10/120 [00:03<00:34,  3.16it/s]\u001b[A\n",
            "Iteration:   9%|▉         | 11/120 [00:03<00:33,  3.23it/s]\u001b[A\n",
            "Iteration:  10%|█         | 12/120 [00:03<00:33,  3.27it/s]\u001b[A\n",
            "Iteration:  11%|█         | 13/120 [00:03<00:32,  3.31it/s]\u001b[A\n",
            "Iteration:  12%|█▏        | 14/120 [00:04<00:31,  3.35it/s]\u001b[A\n",
            "Iteration:  12%|█▎        | 15/120 [00:04<00:31,  3.37it/s]\u001b[A\n",
            "Iteration:  13%|█▎        | 16/120 [00:04<00:31,  3.35it/s]\u001b[A\n",
            "Iteration:  14%|█▍        | 17/120 [00:05<00:30,  3.34it/s]\u001b[A\n",
            "Iteration:  15%|█▌        | 18/120 [00:05<00:30,  3.36it/s]\u001b[A\n",
            "Iteration:  16%|█▌        | 19/120 [00:05<00:29,  3.37it/s]\u001b[A05/06/2020 19:14:32 - INFO - __main__ -   Loading features from cached file ./cached_lm_256_oxxxymiron_lyrics_end_text.txt\n",
            "05/06/2020 19:14:32 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "05/06/2020 19:14:32 - INFO - __main__ -     Num examples = 240\n",
            "05/06/2020 19:14:32 - INFO - __main__ -     Batch size = 2\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/120 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   2%|▏         | 2/120 [00:00<00:09, 12.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3%|▎         | 4/120 [00:00<00:09, 12.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   5%|▌         | 6/120 [00:00<00:08, 12.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   7%|▋         | 8/120 [00:00<00:08, 12.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   8%|▊         | 10/120 [00:00<00:08, 12.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  10%|█         | 12/120 [00:00<00:08, 12.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12%|█▏        | 14/120 [00:01<00:08, 12.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  13%|█▎        | 16/120 [00:01<00:08, 12.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  15%|█▌        | 18/120 [00:01<00:07, 13.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  17%|█▋        | 20/120 [00:01<00:07, 13.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18%|█▊        | 22/120 [00:01<00:07, 13.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  20%|██        | 24/120 [00:01<00:07, 13.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  22%|██▏       | 26/120 [00:01<00:07, 13.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  23%|██▎       | 28/120 [00:02<00:07, 13.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  25%|██▌       | 30/120 [00:02<00:06, 13.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  27%|██▋       | 32/120 [00:02<00:06, 13.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  28%|██▊       | 34/120 [00:02<00:06, 13.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  30%|███       | 36/120 [00:02<00:06, 13.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  32%|███▏      | 38/120 [00:02<00:06, 13.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  33%|███▎      | 40/120 [00:03<00:06, 13.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  35%|███▌      | 42/120 [00:03<00:05, 13.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37%|███▋      | 44/120 [00:03<00:05, 13.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  38%|███▊      | 46/120 [00:03<00:05, 13.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  40%|████      | 48/120 [00:03<00:05, 13.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  42%|████▏     | 50/120 [00:03<00:05, 13.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  43%|████▎     | 52/120 [00:03<00:05, 13.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  45%|████▌     | 54/120 [00:04<00:04, 13.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  47%|████▋     | 56/120 [00:04<00:04, 13.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  48%|████▊     | 58/120 [00:04<00:04, 13.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  50%|█████     | 60/120 [00:04<00:04, 13.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  52%|█████▏    | 62/120 [00:04<00:04, 13.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  53%|█████▎    | 64/120 [00:04<00:04, 13.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 66/120 [00:05<00:04, 13.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 68/120 [00:05<00:03, 13.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 70/120 [00:05<00:03, 13.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 72/120 [00:05<00:03, 13.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 74/120 [00:05<00:03, 13.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 76/120 [00:05<00:03, 13.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 78/120 [00:05<00:03, 13.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 80/120 [00:06<00:03, 13.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 82/120 [00:06<00:02, 13.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 84/120 [00:06<00:02, 13.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 86/120 [00:06<00:02, 13.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 88/120 [00:06<00:02, 13.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 90/120 [00:06<00:02, 13.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 92/120 [00:07<00:02, 13.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 94/120 [00:07<00:01, 13.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 96/120 [00:07<00:01, 13.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 98/120 [00:07<00:01, 13.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 100/120 [00:07<00:01, 13.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 102/120 [00:07<00:01, 13.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 104/120 [00:07<00:01, 13.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 106/120 [00:08<00:01, 13.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 108/120 [00:08<00:00, 13.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 110/120 [00:08<00:00, 13.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 112/120 [00:08<00:00, 13.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 114/120 [00:08<00:00, 13.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 116/120 [00:08<00:00, 13.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 118/120 [00:08<00:00, 13.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 120/120 [00:09<00:00, 13.13it/s]\n",
            "05/06/2020 19:14:41 - INFO - __main__ -   ***** Eval results  *****\n",
            "05/06/2020 19:14:41 - INFO - __main__ -     eval_loss = 1.929780547817548\n",
            "05/06/2020 19:14:41 - INFO - __main__ -     perplexity = tensor(6.8880)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "05/06/2020 19:14:41 - INFO - transformers.configuration_utils -   Configuration saved in ./textgenmodels/checkpoint-50/config.json\n",
            "05/06/2020 19:14:47 - INFO - transformers.modeling_utils -   Model weights saved in ./textgenmodels/checkpoint-50/pytorch_model.bin\n",
            "05/06/2020 19:14:47 - INFO - __main__ -   Saving model checkpoint to ./textgenmodels/checkpoint-50\n",
            "\n",
            "Iteration:  17%|█▋        | 20/120 [00:21<08:02,  4.83s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 21/120 [00:21<05:43,  3.47s/it]\u001b[A\n",
            "Iteration:  18%|█▊        | 22/120 [00:21<04:06,  2.52s/it]\u001b[A\n",
            "Iteration:  19%|█▉        | 23/120 [00:21<02:59,  1.85s/it]\u001b[A\n",
            "Iteration:  20%|██        | 24/120 [00:22<02:13,  1.39s/it]\u001b[A\n",
            "Iteration:  21%|██        | 25/120 [00:22<01:41,  1.06s/it]\u001b[A\n",
            "Iteration:  22%|██▏       | 26/120 [00:22<01:18,  1.20it/s]\u001b[A\n",
            "Iteration:  22%|██▎       | 27/120 [00:23<01:02,  1.49it/s]\u001b[A\n",
            "Iteration:  23%|██▎       | 28/120 [00:23<00:51,  1.79it/s]\u001b[A\n",
            "Iteration:  24%|██▍       | 29/120 [00:23<00:43,  2.09it/s]\u001b[A\n",
            "Iteration:  25%|██▌       | 30/120 [00:24<00:40,  2.23it/s]\u001b[A\n",
            "Iteration:  26%|██▌       | 31/120 [00:24<00:35,  2.47it/s]\u001b[A\n",
            "Iteration:  27%|██▋       | 32/120 [00:24<00:32,  2.69it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 33/120 [00:25<00:30,  2.85it/s]\u001b[A\n",
            "Iteration:  28%|██▊       | 34/120 [00:25<00:28,  3.00it/s]\u001b[A\n",
            "Iteration:  29%|██▉       | 35/120 [00:25<00:27,  3.11it/s]\u001b[A\n",
            "Iteration:  30%|███       | 36/120 [00:25<00:26,  3.20it/s]\u001b[A\n",
            "Iteration:  31%|███       | 37/120 [00:26<00:25,  3.25it/s]\u001b[A\n",
            "Iteration:  32%|███▏      | 38/120 [00:26<00:24,  3.28it/s]\u001b[A\n",
            "Iteration:  32%|███▎      | 39/120 [00:26<00:24,  3.32it/s]\u001b[A\n",
            "Iteration:  33%|███▎      | 40/120 [00:27<00:25,  3.12it/s]\u001b[A\n",
            "Iteration:  34%|███▍      | 41/120 [00:27<00:24,  3.20it/s]\u001b[A\n",
            "Iteration:  35%|███▌      | 42/120 [00:27<00:24,  3.24it/s]\u001b[A\n",
            "Iteration:  36%|███▌      | 43/120 [00:28<00:23,  3.29it/s]\u001b[A\n",
            "Iteration:  37%|███▋      | 44/120 [00:28<00:22,  3.31it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 45/120 [00:28<00:22,  3.32it/s]\u001b[A\n",
            "Iteration:  38%|███▊      | 46/120 [00:28<00:22,  3.34it/s]\u001b[A\n",
            "Iteration:  39%|███▉      | 47/120 [00:29<00:21,  3.35it/s]\u001b[A\n",
            "Iteration:  40%|████      | 48/120 [00:29<00:21,  3.36it/s]\u001b[A\n",
            "Iteration:  41%|████      | 49/120 [00:29<00:21,  3.37it/s]\u001b[A\n",
            "Iteration:  42%|████▏     | 50/120 [00:30<00:22,  3.14it/s]\u001b[A\n",
            "Iteration:  42%|████▎     | 51/120 [00:30<00:21,  3.22it/s]\u001b[A\n",
            "Iteration:  43%|████▎     | 52/120 [00:30<00:20,  3.26it/s]\u001b[A\n",
            "Iteration:  44%|████▍     | 53/120 [00:31<00:20,  3.26it/s]\u001b[A\n",
            "Iteration:  45%|████▌     | 54/120 [00:31<00:20,  3.30it/s]\u001b[A\n",
            "Iteration:  46%|████▌     | 55/120 [00:31<00:19,  3.32it/s]\u001b[A\n",
            "Iteration:  47%|████▋     | 56/120 [00:32<00:19,  3.33it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 57/120 [00:32<00:18,  3.36it/s]\u001b[A\n",
            "Iteration:  48%|████▊     | 58/120 [00:32<00:18,  3.38it/s]\u001b[A\n",
            "Iteration:  49%|████▉     | 59/120 [00:32<00:18,  3.33it/s]\u001b[A\n",
            "Iteration:  50%|█████     | 60/120 [00:33<00:19,  3.12it/s]\u001b[A\n",
            "Iteration:  51%|█████     | 61/120 [00:33<00:18,  3.19it/s]\u001b[A\n",
            "Iteration:  52%|█████▏    | 62/120 [00:33<00:17,  3.25it/s]\u001b[A\n",
            "Iteration:  52%|█████▎    | 63/120 [00:34<00:17,  3.26it/s]\u001b[A\n",
            "Iteration:  53%|█████▎    | 64/120 [00:34<00:16,  3.30it/s]\u001b[A\n",
            "Iteration:  54%|█████▍    | 65/120 [00:34<00:16,  3.33it/s]\u001b[A\n",
            "Iteration:  55%|█████▌    | 66/120 [00:35<00:16,  3.36it/s]\u001b[A\n",
            "Iteration:  56%|█████▌    | 67/120 [00:35<00:15,  3.36it/s]\u001b[A\n",
            "Iteration:  57%|█████▋    | 68/120 [00:35<00:15,  3.35it/s]\u001b[A\n",
            "Iteration:  57%|█████▊    | 69/120 [00:35<00:15,  3.34it/s]\u001b[A\n",
            "Iteration:  58%|█████▊    | 70/120 [00:36<00:15,  3.13it/s]\u001b[A\n",
            "Iteration:  59%|█████▉    | 71/120 [00:36<00:15,  3.19it/s]\u001b[A\n",
            "Iteration:  60%|██████    | 72/120 [00:36<00:14,  3.26it/s]\u001b[A\n",
            "Iteration:  61%|██████    | 73/120 [00:37<00:14,  3.28it/s]\u001b[A\n",
            "Iteration:  62%|██████▏   | 74/120 [00:37<00:13,  3.32it/s]\u001b[A\n",
            "Iteration:  62%|██████▎   | 75/120 [00:37<00:13,  3.34it/s]\u001b[A\n",
            "Iteration:  63%|██████▎   | 76/120 [00:38<00:13,  3.33it/s]\u001b[A\n",
            "Iteration:  64%|██████▍   | 77/120 [00:38<00:12,  3.36it/s]\u001b[A\n",
            "Iteration:  65%|██████▌   | 78/120 [00:38<00:12,  3.38it/s]\u001b[A\n",
            "Iteration:  66%|██████▌   | 79/120 [00:38<00:12,  3.38it/s]\u001b[A\n",
            "Iteration:  67%|██████▋   | 80/120 [00:39<00:12,  3.13it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 81/120 [00:39<00:12,  3.22it/s]\u001b[A\n",
            "Iteration:  68%|██████▊   | 82/120 [00:39<00:11,  3.27it/s]\u001b[A\n",
            "Iteration:  69%|██████▉   | 83/120 [00:40<00:12,  2.96it/s]\u001b[A\n",
            "Iteration:  70%|███████   | 84/120 [00:40<00:11,  3.08it/s]\u001b[A\n",
            "Iteration:  71%|███████   | 85/120 [00:40<00:11,  3.17it/s]\u001b[A\n",
            "Iteration:  72%|███████▏  | 86/120 [00:41<00:10,  3.23it/s]\u001b[A\n",
            "Iteration:  72%|███████▎  | 87/120 [00:41<00:10,  3.28it/s]\u001b[A\n",
            "Iteration:  73%|███████▎  | 88/120 [00:41<00:09,  3.31it/s]\u001b[A\n",
            "Iteration:  74%|███████▍  | 89/120 [00:42<00:09,  3.32it/s]\u001b[A\n",
            "Iteration:  75%|███████▌  | 90/120 [00:42<00:09,  3.12it/s]\u001b[A\n",
            "Iteration:  76%|███████▌  | 91/120 [00:42<00:09,  3.21it/s]\u001b[A\n",
            "Iteration:  77%|███████▋  | 92/120 [00:43<00:08,  3.27it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 93/120 [00:43<00:08,  3.30it/s]\u001b[A\n",
            "Iteration:  78%|███████▊  | 94/120 [00:43<00:07,  3.33it/s]\u001b[A\n",
            "Iteration:  79%|███████▉  | 95/120 [00:43<00:07,  3.34it/s]\u001b[A\n",
            "Iteration:  80%|████████  | 96/120 [00:44<00:07,  3.36it/s]\u001b[A\n",
            "Iteration:  81%|████████  | 97/120 [00:44<00:06,  3.38it/s]\u001b[A\n",
            "Iteration:  82%|████████▏ | 98/120 [00:44<00:06,  3.39it/s]\u001b[A\n",
            "Iteration:  82%|████████▎ | 99/120 [00:45<00:06,  3.38it/s]\u001b[A\n",
            "Iteration:  83%|████████▎ | 100/120 [00:45<00:06,  3.14it/s]\u001b[A\n",
            "Iteration:  84%|████████▍ | 101/120 [00:45<00:05,  3.21it/s]\u001b[A\n",
            "Iteration:  85%|████████▌ | 102/120 [00:46<00:05,  3.26it/s]\u001b[A\n",
            "Iteration:  86%|████████▌ | 103/120 [00:46<00:05,  3.31it/s]\u001b[A\n",
            "Iteration:  87%|████████▋ | 104/120 [00:46<00:04,  3.34it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 105/120 [00:46<00:04,  3.36it/s]\u001b[A\n",
            "Iteration:  88%|████████▊ | 106/120 [00:47<00:04,  3.36it/s]\u001b[A\n",
            "Iteration:  89%|████████▉ | 107/120 [00:47<00:03,  3.37it/s]\u001b[A\n",
            "Iteration:  90%|█████████ | 108/120 [00:47<00:03,  3.38it/s]\u001b[A\n",
            "Iteration:  91%|█████████ | 109/120 [00:48<00:03,  3.40it/s]\u001b[A\n",
            "Iteration:  92%|█████████▏| 110/120 [00:48<00:03,  3.18it/s]\u001b[A\n",
            "Iteration:  92%|█████████▎| 111/120 [00:48<00:02,  3.25it/s]\u001b[A\n",
            "Iteration:  93%|█████████▎| 112/120 [00:49<00:02,  3.30it/s]\u001b[A\n",
            "Iteration:  94%|█████████▍| 113/120 [00:49<00:02,  3.30it/s]\u001b[A\n",
            "Iteration:  95%|█████████▌| 114/120 [00:49<00:01,  3.34it/s]\u001b[A\n",
            "Iteration:  96%|█████████▌| 115/120 [00:49<00:01,  3.35it/s]\u001b[A\n",
            "Iteration:  97%|█████████▋| 116/120 [00:50<00:01,  3.34it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 117/120 [00:50<00:00,  3.35it/s]\u001b[A\n",
            "Iteration:  98%|█████████▊| 118/120 [00:50<00:00,  3.37it/s]\u001b[A\n",
            "Iteration:  99%|█████████▉| 119/120 [00:51<00:00,  3.38it/s]\u001b[A\n",
            "Iteration: 100%|██████████| 120/120 [00:51<00:00,  2.33it/s]\n",
            "Epoch: 100%|██████████| 5/5 [03:16<00:00, 39.36s/it]\n",
            "05/06/2020 19:15:17 - INFO - __main__ -    global_step = 60, average loss = 3.3608702001472315\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58WmWwxiW5NK",
        "colab_type": "code",
        "outputId": "ff4e4bda-0c98-4ff5-9feb-4325bfb2433a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "    # Create output directory if needed\n",
        "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
        "        os.makedirs(args.output_dir)\n",
        "\n",
        "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "    # They can then be reloaded using `from_pretrained()`\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "    # Good practice: save your training arguments together with the trained model\n",
        "    torch.save(parser, os.path.join(args.output_dir, 'training_args.bin'))\n",
        "\n",
        "    # Load a trained model and vocabulary that you have fine-tuned\n",
        "    model = model_class.from_pretrained(args.output_dir)\n",
        "    tokenizer = YTEncoder.from_pretrained(args.output_dir)\n",
        "    model.to(args.device)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "05/06/2020 19:15:17 - INFO - __main__ -   Saving model checkpoint to ./textgenmodels\n",
            "05/06/2020 19:15:17 - INFO - transformers.configuration_utils -   Configuration saved in ./textgenmodels/config.json\n",
            "05/06/2020 19:15:24 - INFO - transformers.modeling_utils -   Model weights saved in ./textgenmodels/pytorch_model.bin\n",
            "05/06/2020 19:15:24 - INFO - transformers.configuration_utils -   loading configuration file ./textgenmodels/config.json\n",
            "05/06/2020 19:15:24 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 1,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 1024,\n",
            "  \"n_head\": 16,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 1024,\n",
            "  \"n_special\": 0,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"predict_special_tokens\": true,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "05/06/2020 19:15:24 - INFO - transformers.modeling_utils -   loading weights file ./textgenmodels/pytorch_model.bin\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiU3UtCZjloQ",
        "colab_type": "code",
        "outputId": "52e297d2-c036-4c02-b8b4-53e026ce665e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "CONTEXT_TEXT = 'Всё ещё бабла нет, всё ещё с долгами канитель \\n \\\n",
        "Все ещё в подвале, всё ещё Parliament на бите \\n \\\n",
        "И я вернусь на трек, твой хуй как Тулуз-Лотрек \\n \\\n",
        "Если русский рэп в гробу сто лет, то я ебу скелет \\n \\\n",
        "И я построил альбом на костях \\n \\\n",
        "Этому не видно конца, как будто он голый толстяк \\n \\\n",
        "Каждый просит фит, каждый пишет: «Денег дам» \\n \\\n",
        "Вас миллион, но мой кумир – Гриша Перельман \\n'\n",
        "\n",
        "START_TEXT = 'Че ты мне скажешь епт '\n",
        "CONTEXT_TEXT = CONTEXT_TEXT + START_TEXT\n",
        "\n",
        "context_tokens = tokenizer.encode(CONTEXT_TEXT)\n",
        "sampled = sample_sequence(model, \n",
        "                          100, \n",
        "                          context_tokens, \n",
        "                          temperature = 1.0,\n",
        "                          top_p=0.99\n",
        "                          )\n",
        "\n",
        "out = sampled[:, len(context_tokens):].tolist()\n",
        "text = ''.join([tokenizer.decode(o) for o in out])\n",
        "\n",
        "# text = text[: text.find('<| endoftext|>')].split('\\n')\n",
        "text = text.split('\\n')\n",
        "\n",
        "print('-' * 20)\n",
        "\n",
        "for i, t in enumerate(text):\n",
        "    if i == 0:\n",
        "        print(START_TEXT + t)\n",
        "    else:\n",
        "        print(t)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:05<00:00, 18.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "Че ты мне скажешь епт т, ты рэп без словаря?\n",
            "Группа учит меня, что слов в словаре не хватает\n",
            "Ты пишешь рэп, как Раунд трип, мой брат – Пиздец\n",
            "Не ссы, всё будет хитом, ты пишешь, как курица лапой\n",
            "Рукой, не понимая слова, ты пытаешься втирать слова\n",
            "Ты будто б школьник в восьмом классе, у тебя даже эротика\n",
            "Ты просто диджей, тебе не по силам райдер\n",
            "Для моих товарищей по несчастью gif\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iekxNtc-qYkV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "5b45c3f7-f9ab-4297-ae09-314176fadc54"
      },
      "source": [
        "# ! rm -rf ./textgenmodels/checkpoint-50\n",
        "# ! zip -r res_oxxxymiron.zip textgenmodels"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: textgenmodels/ (stored 0%)\n",
            "  adding: textgenmodels/training_args.bin (deflated 39%)\n",
            "  adding: textgenmodels/config.json (deflated 57%)\n",
            "  adding: textgenmodels/encoder.model (deflated 57%)\n",
            "  adding: textgenmodels/eval_results.txt (stored 0%)\n",
            "  adding: textgenmodels/pytorch_model.bin (deflated 13%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ29bAwsdekz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "f95347bb-1917-4b83-dcf7-dfb7ea149077"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtXg1Bgtdk4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! cp ./res_oxxxymiron.zip './gdrive/My Drive/gpt2/res_oxxxymiron.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}